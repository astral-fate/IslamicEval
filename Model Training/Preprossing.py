importÂ pandasÂ asÂ pd
importÂ numpyÂ asÂ np
importÂ torch
fromÂ transformersÂ importÂ (
Â Â Â Â AutoTokenizer,
Â Â Â Â AutoModelForTokenClassification,
Â Â Â Â TrainingArguments,
Â Â Â Â Trainer,
Â Â Â Â DataCollatorForTokenClassification,
Â Â Â Â EarlyStoppingCallback
)
fromÂ datasetsÂ importÂ load_from_disk
importÂ re
importÂ os
importÂ zipfile
importÂ time
fromÂ tqdmÂ importÂ tqdm

#Â ---Â 1.Â ConfigurationÂ ---
MODEL_NAMEÂ =Â "aubmindlab/bert-base-arabertv2"
OUTPUT_DIRÂ =Â "/content/drive/MyDrive/FinalIslamic/arabert_finetuned_model_best_v2"
TEST_XML_PATHÂ =Â "test_SubtaskA.xml"

#Â PathsÂ forÂ dataÂ preparedÂ byÂ preprocessing.py
PREPROCESSED_TRAIN_PATHÂ =Â "/content/drive/MyDrive/FinalIslamic/prepros/preprocessed_train_dataset"
PREPROCESSED_VAL_PATHÂ =Â "/content/drive/MyDrive/FinalIslamic/prepros/preprocessed_val_dataset"

#Â OutputÂ pathsÂ forÂ submission
SUBMISSION_TSV_PATHÂ =Â "/content/drive/MyDrive/FinalIslamic/test/submission.tsv"
SUBMISSION_ZIP_PATHÂ =Â "/content/drive/MyDrive/FinalIslamic/test/submission.zip"

defÂ fast_train_model():
Â Â Â Â """FastÂ trainingÂ usingÂ preprocessedÂ dataÂ fromÂ disk."""
Â Â Â Â print("ğŸš€Â STEPÂ 2:Â FASTÂ TRAINING")
Â Â Â Â print("="Â *Â 50)

Â Â Â Â ifÂ notÂ os.path.exists(PREPROCESSED_TRAIN_PATH)Â orÂ notÂ os.path.exists(PREPROCESSED_VAL_PATH):
Â Â Â Â Â Â Â Â print(f"âŒÂ PreprocessedÂ datasetsÂ notÂ foundÂ atÂ {PREPROCESSED_TRAIN_PATH}")
Â Â Â Â Â Â Â Â print("PleaseÂ runÂ theÂ 'preprocessing.py'Â scriptÂ first.")
Â Â Â Â Â Â Â Â returnÂ False

Â Â Â Â print("ğŸ“¥Â LoadingÂ preprocessedÂ datasets...")
Â Â Â Â train_datasetÂ =Â load_from_disk(PREPROCESSED_TRAIN_PATH)
Â Â Â Â val_datasetÂ =Â load_from_disk(PREPROCESSED_VAL_PATH)
Â Â Â Â print(f"âœ…Â LoadedÂ {len(train_dataset)}Â trainingÂ andÂ {len(val_dataset)}Â validationÂ examples.")

Â Â Â Â label_listÂ =Â ['O',Â 'B-Ayah',Â 'I-Ayah',Â 'B-Hadith',Â 'I-Hadith']
Â Â Â Â id_to_labelÂ =Â {i:Â lÂ forÂ i,Â lÂ inÂ enumerate(label_list)}
Â Â Â Â label_to_idÂ =Â {l:Â iÂ forÂ i,Â lÂ inÂ enumerate(label_list)}

Â Â Â Â modelÂ =Â AutoModelForTokenClassification.from_pretrained(
Â Â Â Â Â Â Â Â MODEL_NAME,Â num_labels=len(label_list),Â id2label=id_to_label,Â label2id=label_to_id
Â Â Â Â )
Â Â Â Â tokenizerÂ =Â AutoTokenizer.from_pretrained(MODEL_NAME)

Â Â Â Â training_argsÂ =Â TrainingArguments(
Â Â Â Â Â Â Â Â output_dir=OUTPUT_DIR,
Â Â Â Â Â Â Â Â num_train_epochs=6,
Â Â Â Â Â Â Â Â per_device_train_batch_size=16,
Â Â Â Â Â Â Â Â gradient_accumulation_steps=2,
Â Â Â Â Â Â Â Â learning_rate=3e-5,
Â Â Â Â Â Â Â Â fp16=True,
Â Â Â Â Â Â Â Â logging_steps=100,
Â Â Â Â Â Â Â Â eval_strategy="epoch",
Â Â Â Â Â Â Â Â save_strategy="epoch",
Â Â Â Â Â Â Â Â load_best_model_at_end=True,
Â Â Â Â Â Â Â Â metric_for_best_model="eval_loss",
Â Â Â Â Â Â Â Â greater_is_better=False,
Â Â Â Â Â Â Â Â save_total_limit=2,
Â Â Â Â Â Â Â Â report_to="none",
Â Â Â Â )

Â Â Â Â trainerÂ =Â Trainer(
Â Â Â Â Â Â Â Â model=model,
Â Â Â Â Â Â Â Â args=training_args,
Â Â Â Â Â Â Â Â train_dataset=train_dataset,
Â Â Â Â Â Â Â Â eval_dataset=val_dataset,
Â Â Â Â Â Â Â Â tokenizer=tokenizer,
Â Â Â Â Â Â Â Â data_collator=DataCollatorForTokenClassification(tokenizer),
Â Â Â Â Â Â Â Â callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]
Â Â Â Â )

Â Â Â Â print("ğŸƒâ€â™‚ï¸Â StartingÂ training...")
Â Â Â Â start_timeÂ =Â time.time()
Â Â Â Â trainer.train()
Â Â Â Â print(f"â±ï¸Â TrainingÂ completedÂ inÂ {(time.time()Â -Â start_time)/60:.1f}Â minutes")

Â Â Â Â trainer.save_model(OUTPUT_DIR)
Â Â Â Â print(f"âœ…Â BestÂ modelÂ savedÂ toÂ {OUTPUT_DIR}")
Â Â Â Â returnÂ True

defÂ load_test_data_from_xml(xml_path):
Â Â Â Â """LoadsÂ testÂ dataÂ fromÂ theÂ providedÂ XMLÂ fileÂ format."""
Â Â Â Â ifÂ notÂ os.path.exists(xml_path):
Â Â Â Â Â Â Â Â print(f"âŒÂ TestÂ fileÂ notÂ foundÂ atÂ {xml_path}")
Â Â Â Â Â Â Â Â returnÂ []
Â Â Â Â withÂ open(xml_path,Â 'r',Â encoding='utf-8')Â asÂ f:
Â Â Â Â Â Â Â Â contentÂ =Â f.read()
Â Â Â Â patternÂ =Â re.compile(r"<Question>.*?<ID>(.*?)</ID>.*?<Response>(.*?)</Response>.*?</Question>",Â re.DOTALL)
Â Â Â Â matchesÂ =Â pattern.findall(content)
Â Â Â Â returnÂ [{'Question_ID':Â m[0].strip(),Â 'Text':Â m[1].strip()}Â forÂ mÂ inÂ matches]

defÂ predict_on_test_data(model,Â tokenizer,Â test_data):
Â Â Â Â """PredictsÂ spansÂ onÂ theÂ testÂ data."""
Â Â Â Â print("ğŸ”®Â PredictingÂ spansÂ onÂ testÂ set...")
Â Â Â Â model.eval()
Â Â Â Â deviceÂ =Â "cuda"Â ifÂ torch.cuda.is_available()Â elseÂ "cpu"
Â Â Â Â model.to(device)
Â Â Â Â print(f"UsingÂ device:Â {device}")

Â Â Â Â label_listÂ =Â list(model.config.id2label.values())
Â Â Â Â all_predictionsÂ =Â []

Â Â Â Â forÂ itemÂ inÂ tqdm(test_data,Â desc="Predicting"):
Â Â Â Â Â Â Â Â qid,Â textÂ =Â item["Question_ID"],Â item["Text"]
Â Â Â Â Â Â Â Â ifÂ notÂ text.strip():
Â Â Â Â Â Â Â Â Â Â Â Â all_predictions.append({"Question_ID":Â qid,Â "Span_Start":Â 0,Â "Span_End":Â 0,Â "Span_Type":Â "No_Spans"})
Â Â Â Â Â Â Â Â Â Â Â Â continue

Â Â Â Â Â Â Â Â inputsÂ =Â tokenizer(text,Â return_tensors="pt",Â truncation=True,Â max_length=512).to(device)
Â Â Â Â Â Â Â Â withÂ torch.no_grad():
Â Â Â Â Â Â Â Â Â Â Â Â logitsÂ =Â model(**inputs).logits

Â Â Â Â Â Â Â Â predsÂ =Â torch.argmax(logits,Â dim=2)[0].cpu().numpy()
Â Â Â Â Â Â Â Â spansÂ =Â []
Â Â Â Â Â Â Â Â current_spanÂ =Â None
Â Â Â Â Â Â Â Â forÂ i,Â pred_idÂ inÂ enumerate(preds):
Â Â Â Â Â Â Â Â Â Â Â Â labelÂ =Â label_list[pred_id]
Â Â Â Â Â Â Â Â Â Â Â Â word_idÂ =Â inputs.word_ids(batch_index=0)[i]
Â Â Â Â Â Â Â Â Â Â Â Â ifÂ word_idÂ isÂ None:Â continue

Â Â Â Â Â Â Â Â Â Â Â Â ifÂ label.startswith('B-'):
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â ifÂ current_span:Â spans.append(current_span)
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â csÂ =Â inputs.token_to_chars(i)
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â current_spanÂ =Â {'type':Â label[2:],Â 'start':Â cs.start,Â 'end':Â cs.end}
Â Â Â Â Â Â Â Â Â Â Â Â elifÂ label.startswith('I-')Â andÂ current_spanÂ andÂ current_span['type']Â ==Â label[2:]:
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â csÂ =Â inputs.token_to_chars(i)
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â current_span['end']Â =Â cs.end
Â Â Â Â Â Â Â Â Â Â Â Â elifÂ current_span:
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â spans.append(current_span)
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â current_spanÂ =Â None
Â Â Â Â Â Â Â Â ifÂ current_span:
Â Â Â Â Â Â Â Â Â Â Â Â spans.append(current_span)

Â Â Â Â Â Â Â Â ifÂ spans:
Â Â Â Â Â Â Â Â Â Â Â Â forÂ spanÂ inÂ spans:
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â all_predictions.append({"Question_ID":Â qid,Â "Span_Start":Â span['start'],Â "Span_End":Â span['end'],Â "Span_Type":Â span['type']})
Â Â Â Â Â Â Â Â else:
Â Â Â Â Â Â Â Â Â Â Â Â all_predictions.append({"Question_ID":Â qid,Â "Span_Start":Â 0,Â "Span_End":Â 0,Â "Span_Type":Â "No_Spans"})

Â Â Â Â returnÂ all_predictions

defÂ generate_submission_file(predictions,Â output_path,Â zip_path):
Â Â Â Â """GeneratesÂ theÂ finalÂ submission.tsvÂ andÂ submission.zipÂ files."""
Â Â Â Â print(f"ğŸ“¦Â GeneratingÂ submissionÂ fileÂ atÂ {output_path}...")
Â Â Â Â dfÂ =Â pd.DataFrame(predictions)[["Question_ID",Â "Span_Start",Â "Span_End",Â "Span_Type"]]
Â Â Â Â os.makedirs(os.path.dirname(output_path),Â exist_ok=True)
Â Â Â Â df.to_csv(output_path,Â sep='\t',Â index=False,Â header=False)
Â Â Â Â withÂ zipfile.ZipFile(zip_path,Â 'w')Â asÂ zf:
Â Â Â Â Â Â Â Â zf.write(output_path,Â os.path.basename(output_path))
Â Â Â Â print(f"âœ…Â SubmissionÂ zipÂ createdÂ successfullyÂ atÂ {zip_path}")


defÂ main_finetuning():
Â Â Â Â """MainÂ functionÂ toÂ runÂ trainingÂ andÂ prediction."""
Â Â Â Â #Â ---Â TrainingÂ ---
Â Â Â Â ifÂ notÂ os.path.exists(OUTPUT_DIR):
Â Â Â Â Â Â Â Â ifÂ notÂ fast_train_model():
Â Â Â Â Â Â Â Â Â Â Â Â print("âŒÂ TrainingÂ failed.Â Exiting.")
Â Â Â Â Â Â Â Â Â Â Â Â return
Â Â Â Â else:
Â Â Â Â Â Â Â Â print(f"âœ…Â FoundÂ existingÂ fine-tunedÂ modelÂ atÂ {OUTPUT_DIR}.Â SkippingÂ training.")

Â Â Â Â #Â ---Â PredictionÂ ---
Â Â Â Â print("\n"Â +Â "="*50)
Â Â Â Â print("ğŸš€Â STEPÂ 3:Â PREDICTION")
Â Â Â Â print("="*50)
Â Â Â Â modelÂ =Â AutoModelForTokenClassification.from_pretrained(OUTPUT_DIR)
Â Â Â Â tokenizerÂ =Â AutoTokenizer.from_pretrained(OUTPUT_DIR)

Â Â Â Â test_dataÂ =Â load_test_data_from_xml(TEST_XML_PATH)
Â Â Â Â ifÂ test_data:
Â Â Â Â Â Â Â Â predictionsÂ =Â predict_on_test_data(model,Â tokenizer,Â test_data)
Â Â Â Â Â Â Â Â generate_submission_file(predictions,Â SUBMISSION_TSV_PATH,Â SUBMISSION_ZIP_PATH)
Â Â Â Â Â Â Â Â print("\nğŸ‰Â FullÂ pipelineÂ completedÂ successfully!")
Â Â Â Â else:
Â Â Â Â Â Â Â Â print("âŒÂ CouldÂ notÂ loadÂ testÂ data.Â PredictionÂ stepÂ skipped.")


ifÂ __name__Â ==Â "__main__":
Â  Â  main_finetuning()
