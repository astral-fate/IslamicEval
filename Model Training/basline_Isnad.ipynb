{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8d2b826305df4819b5cc656b034e4e75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d72929a84a0f47abbd1beb4aa28327c1",
              "IPY_MODEL_a320786042934d4da2bd122869964c29",
              "IPY_MODEL_c91d6542f0884720bddcecef5d0a57b1"
            ],
            "layout": "IPY_MODEL_9f3a9ca99ad0474a8eba2a3fead3cc6b"
          }
        },
        "d72929a84a0f47abbd1beb4aa28327c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d3af2a420ea40deb6ef95bee5459daf",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_1485bf2444e74983946e6a1cd0f88cf3",
            "value": "Savingâ€‡theâ€‡datasetâ€‡(1/1â€‡shards):â€‡100%"
          }
        },
        "a320786042934d4da2bd122869964c29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ba12136699194ab89987ebf079a556c4",
            "max": 93099,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a9bcbf86f02d431f91f392f2813a730c",
            "value": 93099
          }
        },
        "c91d6542f0884720bddcecef5d0a57b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0e3b659106d64ff0b2e0659a27f72da0",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_a81325e8a6b548568496613873ea8002",
            "value": "â€‡93099/93099â€‡[00:00&lt;00:00,â€‡421166.72â€‡examples/s]"
          }
        },
        "9f3a9ca99ad0474a8eba2a3fead3cc6b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d3af2a420ea40deb6ef95bee5459daf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1485bf2444e74983946e6a1cd0f88cf3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ba12136699194ab89987ebf079a556c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9bcbf86f02d431f91f392f2813a730c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0e3b659106d64ff0b2e0659a27f72da0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a81325e8a6b548568496613873ea8002": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "355b7ec18502497ab94aae741559a58a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_adc79127f918417a8c682c599bbf849d",
              "IPY_MODEL_dd22920dd039456393b107ccd2c7fec5",
              "IPY_MODEL_049b3d33d9cf46cd8b660cf55b737abe"
            ],
            "layout": "IPY_MODEL_55ddd6da80ca4070b638e176592ba983"
          }
        },
        "adc79127f918417a8c682c599bbf849d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a0f31630efff4c49aa586399a2538c19",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_60adc7da881b4947a4195c28749424ad",
            "value": "Savingâ€‡theâ€‡datasetâ€‡(1/1â€‡shards):â€‡100%"
          }
        },
        "dd22920dd039456393b107ccd2c7fec5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_57c426587fb14b99977591d3f8902600",
            "max": 40626,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_99f2790449e447f493dfa2fab03842dc",
            "value": 40626
          }
        },
        "049b3d33d9cf46cd8b660cf55b737abe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_46464d0834ed47da810d22f4bad46478",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_f754d112118a481f949e01f2610fd497",
            "value": "â€‡40626/40626â€‡[00:00&lt;00:00,â€‡452444.38â€‡examples/s]"
          }
        },
        "55ddd6da80ca4070b638e176592ba983": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0f31630efff4c49aa586399a2538c19": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60adc7da881b4947a4195c28749424ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "57c426587fb14b99977591d3f8902600": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99f2790449e447f493dfa2fab03842dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "46464d0834ed47da810d22f4bad46478": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f754d112118a481f949e01f2610fd497": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "58b6c73a7f1746beb1f5dc080a4261f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7c8efaa094844de2a0f7e4d172b4a3b8",
              "IPY_MODEL_b12e589a721e443b931ec33b31b1517c",
              "IPY_MODEL_ef56dd3d7dfb4689bbf0fa2d9eb7cc53"
            ],
            "layout": "IPY_MODEL_7068950b4fa84c6899e9b47efb2fcfc9"
          }
        },
        "7c8efaa094844de2a0f7e4d172b4a3b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f9c088f18f14a3985d66a16bc9d060e",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_1c0355ea78354151bf298baeef176c20",
            "value": "model.safetensors:â€‡100%"
          }
        },
        "b12e589a721e443b931ec33b31b1517c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_137fdb617bb54723931ccdf6a500746b",
            "max": 543432324,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0674d681a7b640b1b8db3d8b8738c0dc",
            "value": 543432324
          }
        },
        "ef56dd3d7dfb4689bbf0fa2d9eb7cc53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_46826a12c2cf4a719d261f7f0c9d47e7",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_2345fec650f744eda864758488e3ea1d",
            "value": "â€‡543M/543Mâ€‡[00:02&lt;00:00,â€‡487MB/s]"
          }
        },
        "7068950b4fa84c6899e9b47efb2fcfc9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f9c088f18f14a3985d66a16bc9d060e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c0355ea78354151bf298baeef176c20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "137fdb617bb54723931ccdf6a500746b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0674d681a7b640b1b8db3d8b8738c0dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "46826a12c2cf4a719d261f7f0c9d47e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2345fec650f744eda864758488e3ea1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8seLkZ8DiiV",
        "outputId": "f93e90a8-16e0-4266-a060-cb4de11e7b0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import json\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "import re\n",
        "import os\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer\n",
        "import nltk\n",
        "\n",
        "# --- 1. Configuration ---\n",
        "MODEL_NAME = \"aubmindlab/bert-base-arabertv2\"\n",
        "\n",
        "# Input data paths\n",
        "QURAN_JSON_PATH = \"/content/drive/MyDrive/FinalIslamic/data/quran.json\"\n",
        "SIX_HADITH_BOOKS_JSON_PATH = \"/content/drive/MyDrive/FinalIslamic/data/six_hadith_books.json\"\n",
        "\n",
        "# Output paths for processed data\n",
        "PREPROCESSED_TRAIN_PATH = \"/content/drive/MyDrive/FinalIslamic/prepros/preprocessed_train_30p_dataset\"\n",
        "PREPROCESSED_VAL_PATH = \"/content/drive/MyDrive/FinalIslamic/prepros/preprocessed_val_30p_dataset\"\n",
        "CSV_OUTPUT_DIR = \"/content/drive/MyDrive/FinalIslamic/preprocessed_csv_30p/\"\n",
        "\n",
        "\n",
        "# --- NEW HELPER FUNCTION ---\n",
        "def normalize_arabic(text):\n",
        "    \"\"\"Removes Arabic diacritics (Tashkeel) and Tatweel from the text.\"\"\"\n",
        "    # This regex targets the Unicode range for Arabic diacritics and the Tatweel character.\n",
        "    text = re.sub(r'[\\u064B-\\u0652\\u0640]', '', text)\n",
        "    return text\n",
        "\n",
        "\n",
        "def split_long_texts(texts, tokenizer, max_tokens=25, label_type=\"Ayah\"):\n",
        "    \"\"\"\n",
        "    Splits long texts into smaller chunks based purely on token length.\n",
        "    It finds the nearest space to the middle of the text to create a clean split.\n",
        "    \"\"\"\n",
        "    print(f\"ðŸ”ª Splitting {label_type} texts longer than {max_tokens} tokens...\")\n",
        "    split_texts = []\n",
        "    split_count = 0\n",
        "    for text in tqdm(texts, desc=f\"Processing {label_type}s\"):\n",
        "        tokens = tokenizer.tokenize(text)\n",
        "        if len(tokens) <= max_tokens:\n",
        "            split_texts.append(text)\n",
        "        else:\n",
        "            mid_point = len(text) // 2\n",
        "            split_pos = text.rfind(' ', 0, mid_point)\n",
        "            if split_pos == -1:\n",
        "                split_pos = mid_point\n",
        "\n",
        "            part1 = text[:split_pos].strip()\n",
        "            part2 = text[split_pos:].strip()\n",
        "\n",
        "            if part1: split_texts.append(part1)\n",
        "            if part2: split_texts.append(part2)\n",
        "            split_count += 1\n",
        "\n",
        "    print(f\"âœ… Splitting complete. Original: {len(texts)} texts, New total: {len(split_texts)} texts. ({split_count} texts were split).\")\n",
        "    return split_texts\n",
        "\n",
        "\n",
        "def _create_example_fixed(text, label_type, tokenizer, label_to_id, prefixes, suffixes, neutral_sentences, save_details=False):\n",
        "    \"\"\"Creates a single tokenized example with context.\"\"\"\n",
        "    try:\n",
        "        cleaned_text = re.sub(r'\\s+', ' ', text).strip()\n",
        "        if not cleaned_text:\n",
        "            return None\n",
        "\n",
        "        prefix = random.choice(prefixes)\n",
        "        suffix = random.choice(suffixes)\n",
        "\n",
        "        if random.random() > 0.3:\n",
        "            context = random.choice(neutral_sentences)\n",
        "            if random.random() > 0.5:\n",
        "                full_text = f'{prefix} {context} \"{cleaned_text}\" {suffix}'\n",
        "            else:\n",
        "                full_text = f'{prefix} \"{cleaned_text}\" {context} {suffix}'\n",
        "        else:\n",
        "            full_text = f'{prefix} \"{cleaned_text}\" {suffix}'\n",
        "\n",
        "        full_text = re.sub(r'\\s+', ' ', full_text).strip()\n",
        "        char_start = full_text.find(cleaned_text)\n",
        "        if char_start == -1:\n",
        "            return None\n",
        "        char_end = char_start + len(cleaned_text)\n",
        "\n",
        "        tokenized_input = tokenizer(full_text, truncation=True, max_length=512)\n",
        "        input_ids = tokenized_input['input_ids']\n",
        "        attention_mask = tokenized_input['attention_mask']\n",
        "        labels = [label_to_id['O']] * len(input_ids)\n",
        "\n",
        "        start_token = tokenized_input.char_to_token(char_start)\n",
        "        end_token = tokenized_input.char_to_token(char_end - 1)\n",
        "\n",
        "        if start_token is not None and end_token is not None:\n",
        "            labels[start_token] = label_to_id[f'B-{label_type}']\n",
        "            for i in range(start_token + 1, min(end_token + 1, len(labels))):\n",
        "                labels[i] = label_to_id[f'I-{label_type}']\n",
        "\n",
        "        word_ids = tokenized_input.word_ids()\n",
        "        final_labels = []\n",
        "        for i, word_id in enumerate(word_ids):\n",
        "            if word_id is None or (i > 0 and word_id == word_ids[i - 1]):\n",
        "                final_labels.append(-100)\n",
        "            else:\n",
        "                final_labels.append(labels[i] if i < len(labels) else label_to_id['O'])\n",
        "\n",
        "        result = {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"labels\": final_labels\n",
        "        }\n",
        "\n",
        "        if save_details:\n",
        "            result.update({\n",
        "                \"original_text\": text,\n",
        "                \"full_text\": full_text,\n",
        "                \"prefix\": prefix,\n",
        "                \"suffix\": suffix,\n",
        "                \"char_start\": char_start,\n",
        "                \"char_end\": char_end,\n",
        "                \"label_type\": label_type,\n",
        "                \"target_span\": cleaned_text\n",
        "            })\n",
        "        return result\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def create_validation_examples(tokenizer, label_to_id, val_ayah_texts, val_hadith_texts):\n",
        "    \"\"\"Creates validation examples using a different set of patterns to test generalization.\"\"\"\n",
        "    print(\"ðŸ”„ Creating generalization-focused validation examples...\")\n",
        "\n",
        "    val_ayah_prefixes = [\"\", \"ÙˆÙÙŠ Ø§Ù„Ù‚Ø±Ø¢Ù† Ø§Ù„ÙƒØ±ÙŠÙ… Ù†Ø¬Ø¯:\", \"ÙˆÙ…Ù† Ø¢ÙŠØ§Øª Ø§Ù„Ù„Ù‡:\", \"ÙˆÙ‚Ø¯ Ø£Ù†Ø²Ù„ Ø§Ù„Ù„Ù‡:\", \"ÙˆÙŠÙ‚ÙˆÙ„ Ø§Ù„Ø­Ù‚ ØªØ¨Ø§Ø±Ùƒ ÙˆØªØ¹Ø§Ù„Ù‰:\", \"ÙˆÙÙŠ Ø§Ù„Ø°ÙƒØ± Ø§Ù„Ø­ÙƒÙŠÙ…:\", \"ÙˆÙÙŠ ÙƒØªØ§Ø¨ Ø§Ù„Ù„Ù‡ Ù†Ù‚Ø±Ø£:\", \"ÙˆØ§Ù„Ø¯Ù„ÙŠÙ„ Ø¹Ù„Ù‰ Ø°Ù„Ùƒ Ù‚ÙˆÙ„Ù‡ ØªØ¹Ø§Ù„Ù‰:\"]\n",
        "    val_ayah_suffixes = [\"\", \"Ù‡Ø°Ø§ Ù…Ù† ÙƒÙ„Ø§Ù… Ø§Ù„Ù„Ù‡\", \"Ø¢ÙŠØ© Ø¹Ø¸ÙŠÙ…Ø©\", \"Ù…Ù† Ø§Ù„Ù‚Ø±Ø¢Ù† Ø§Ù„ÙƒØ±ÙŠÙ…\", \"ÙƒÙ„Ø§Ù… Ø±Ø¨ Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠÙ†\", \"Ù…Ù† Ø§Ù„Ø°ÙƒØ± Ø§Ù„Ø­ÙƒÙŠÙ…\", \"Ø¢ÙŠØ© ÙƒØ±ÙŠÙ…Ø©\", \"(ØµØ¯Ù‚ Ø§Ù„Ù„Ù‡ Ø§Ù„Ø¹Ø¸ÙŠÙ…)\"]\n",
        "    val_hadith_prefixes = [\"\", \"ÙˆÙÙŠ Ø§Ù„Ø³Ù†Ø© Ø§Ù„Ù†Ø¨ÙˆÙŠØ©:\", \"ÙˆÙ…Ù† Ù‡Ø¯ÙŠ Ø§Ù„Ù†Ø¨ÙŠ ØµÙ„Ù‰ Ø§Ù„Ù„Ù‡ Ø¹Ù„ÙŠÙ‡ ÙˆØ³Ù„Ù…:\", \"ÙˆÙ‚Ø¯ Ø¹Ù„Ù…Ù†Ø§ Ø§Ù„Ø±Ø³ÙˆÙ„ ØµÙ„Ù‰ Ø§Ù„Ù„Ù‡ Ø¹Ù„ÙŠÙ‡ ÙˆØ³Ù„Ù…:\", \"ÙˆÙÙŠ Ø§Ù„Ø­Ø¯ÙŠØ« Ø§Ù„Ø´Ø±ÙŠÙ Ù†Ø¬Ø¯:\", \"ÙƒÙ…Ø§ Ø¬Ø§Ø¡ ÙÙŠ Ø§Ù„Ø­Ø¯ÙŠØ«:\"]\n",
        "    val_hadith_suffixes = [\"\", \"Ù…Ù† Ø§Ù„Ø³Ù†Ø© Ø§Ù„Ù†Ø¨ÙˆÙŠØ©\", \"Ø­Ø¯ÙŠØ« Ù†Ø¨ÙˆÙŠ Ø´Ø±ÙŠÙ\", \"Ù…Ù† Ù‡Ø¯ÙŠ Ø§Ù„Ù…ØµØ·ÙÙ‰\", \"ØµÙ„Ù‰ Ø§Ù„Ù„Ù‡ Ø¹Ù„ÙŠÙ‡ ÙˆØ³Ù„Ù…\", \"(Ø±ÙˆØ§Ù‡ Ø§Ù„ØªØ±Ù…Ø°ÙŠ)\"]\n",
        "    val_transitions = [\"ÙˆÙ„Ù†ØªØ£Ù…Ù„ Ù…Ø¹Ø§Ù‹\", \"ÙˆÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ø³ÙŠØ§Ù‚\", \"ÙˆÙ„Ù„ØªÙˆØ¶ÙŠØ­\", \"ÙˆØ¥Ù„ÙŠÙƒÙ… Ø§Ù„Ù…Ø«Ø§Ù„\", \"ÙˆÙÙŠ Ù‡Ø°Ø§ Ø§Ù„ØµØ¯Ø¯\", \"ÙˆÙ‡Ø°Ø§ ÙŠØ¨ÙŠÙ† Ù„Ù†Ø§ Ø£Ù‡Ù…ÙŠØ© Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹.\"]\n",
        "\n",
        "    validation_data = []\n",
        "    validation_csv_data = []\n",
        "\n",
        "    for ayah in tqdm(val_ayah_texts, desc=\"Val Ayahs\"):\n",
        "        for variation_num in range(3):\n",
        "            example = _create_example_fixed(ayah, 'Ayah', tokenizer, label_to_id, val_ayah_prefixes, val_ayah_suffixes, val_transitions, save_details=True)\n",
        "            if example:\n",
        "                validation_data.append({k: v for k, v in example.items() if k in [\"input_ids\", \"attention_mask\", \"labels\"]})\n",
        "                details = {k: v for k, v in example.items() if k not in [\"input_ids\", \"attention_mask\", \"labels\"]}\n",
        "                details.update({\"variation_number\": variation_num + 1, \"dataset_split\": \"validation\"})\n",
        "                validation_csv_data.append(details)\n",
        "\n",
        "    for hadith in tqdm(val_hadith_texts, desc=\"Val Hadiths\"):\n",
        "        for variation_num in range(3):\n",
        "            example = _create_example_fixed(hadith, 'Hadith', tokenizer, label_to_id, val_hadith_prefixes, val_hadith_suffixes, val_transitions, save_details=True)\n",
        "            if example:\n",
        "                validation_data.append({k: v for k, v in example.items() if k in [\"input_ids\", \"attention_mask\", \"labels\"]})\n",
        "                details = {k: v for k, v in example.items() if k not in [\"input_ids\", \"attention_mask\", \"labels\"]}\n",
        "                details.update({\"variation_number\": variation_num + 1, \"dataset_split\": \"validation\"})\n",
        "                validation_csv_data.append(details)\n",
        "\n",
        "    print(f\"âœ… Created {len(validation_data)} validation examples.\")\n",
        "    return validation_data, validation_csv_data\n",
        "\n",
        "\n",
        "def main_preprocessing():\n",
        "    \"\"\"Main function to run the entire preprocessing pipeline.\"\"\"\n",
        "    print(\"ðŸ”„ STEP 1: OFFLINE PREPROCESSING WITH 30% BALANCED VALIDATION\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    os.makedirs(CSV_OUTPUT_DIR, exist_ok=True)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "    label_list = ['O', 'B-Ayah', 'I-Ayah', 'B-Hadith', 'I-Hadith']\n",
        "    label_to_id = {l: i for i, l in enumerate(label_list)}\n",
        "\n",
        "    print(\"Loading raw data...\")\n",
        "    with open(QURAN_JSON_PATH, 'r', encoding='utf-8') as f:\n",
        "        quran_data = json.load(f)\n",
        "    with open(SIX_HADITH_BOOKS_JSON_PATH, 'r', encoding='utf-8') as f:\n",
        "        six_books_data = json.load(f)\n",
        "\n",
        "    ayah_texts = [item['ayah_text'] for item in quran_data if 'ayah_text' in item]\n",
        "    hadith_texts = [item['Matn'].strip() for item in six_books_data if 'Matn' in item and item['Matn'] and item['Matn'].strip()]\n",
        "\n",
        "    # Step 1: Split long texts\n",
        "    ayah_texts = split_long_texts(ayah_texts, tokenizer, max_tokens=25, label_type=\"Ayah\")\n",
        "\n",
        "    # --- NEW: NORMALIZE AND AUGMENT AYAH DATA ---\n",
        "    print(\"ðŸ”„ Normalizing Ayah texts for data augmentation...\")\n",
        "    # Create a new list containing Ayahs with Tashkeel removed\n",
        "    normalized_ayah_texts = [normalize_arabic(text) for text in tqdm(ayah_texts, desc=\"Normalizing\")]\n",
        "\n",
        "    # Combine the original (with Tashkeel) and normalized (without Tashkeel) lists\n",
        "    original_count = len(ayah_texts)\n",
        "    ayah_texts.extend(normalized_ayah_texts)\n",
        "    print(f\"âœ… Normalization complete. Ayah count increased from {original_count} to {len(ayah_texts)}.\")\n",
        "    # --- END OF NEW LOGIC ---\n",
        "\n",
        "    MAX_TEXT_LENGTH = 1500\n",
        "    ayah_texts = [t for t in ayah_texts if len(t) < MAX_TEXT_LENGTH]\n",
        "    hadith_texts = [t for t in hadith_texts if len(t) < MAX_TEXT_LENGTH]\n",
        "    print(f\"Filtered: {len(ayah_texts)} Ayahs, {len(hadith_texts)} Hadiths\")\n",
        "\n",
        "    # --- MODIFIED: 30% BALANCED VALIDATION SPLIT ---\n",
        "    random.seed(42)\n",
        "\n",
        "    # Calculate validation size based on 30% of the total unique texts\n",
        "    total_texts = len(ayah_texts) + len(hadith_texts)\n",
        "    total_val_size = int(total_texts * 0.30)\n",
        "    # Ensure the total size is an even number for a perfect 50/50 split\n",
        "    if total_val_size % 2 != 0:\n",
        "        total_val_size += 1\n",
        "    val_size_per_class = total_val_size // 2\n",
        "\n",
        "    print(f\"ðŸŽ¯ Creating 30% BALANCED validation split:\")\n",
        "    print(f\"   - Total available texts: {total_texts:,}\")\n",
        "    print(f\"   - Target validation size (30%): {total_val_size:,} texts ({val_size_per_class} per class)\")\n",
        "    print(f\"   - Target validation examples (x3): {total_val_size * 3:,} examples\")\n",
        "    print(f\"   - Available Ayah texts: {len(ayah_texts):,}\")\n",
        "    print(f\"   - Available Hadith texts: {len(hadith_texts):,}\")\n",
        "\n",
        "    # Ensure we have enough texts in each class\n",
        "    if len(ayah_texts) < val_size_per_class:\n",
        "        print(f\"âŒ WARNING: Not enough Ayah texts for a balanced 30% split!\")\n",
        "        print(f\"   - Need {val_size_per_class}, have {len(ayah_texts)}. Adjusting validation size.\")\n",
        "        val_size_per_class = len(ayah_texts)\n",
        "        total_val_size = val_size_per_class * 2\n",
        "        print(f\"   - Reduced validation size to: {total_val_size} texts ({val_size_per_class} per class)\")\n",
        "\n",
        "    if len(hadith_texts) < val_size_per_class:\n",
        "        print(f\"âŒ WARNING: Not enough Hadith texts for a balanced 30% split!\")\n",
        "        print(f\"   - Need {val_size_per_class}, have {len(hadith_texts)}. Adjusting validation size.\")\n",
        "        val_size_per_class = min(val_size_per_class, len(hadith_texts))\n",
        "        total_val_size = val_size_per_class * 2\n",
        "        print(f\"   - Reduced validation size to: {total_val_size} texts ({val_size_per_class} per class)\")\n",
        "\n",
        "    # Sample equal numbers from each class\n",
        "    val_ayah_texts = random.sample(ayah_texts, val_size_per_class)\n",
        "    val_hadith_texts = random.sample(hadith_texts, val_size_per_class)\n",
        "\n",
        "    print(f\"âœ… 30% balanced validation split created:\")\n",
        "    print(f\"   - Validation Ayah texts: {len(val_ayah_texts):,}\")\n",
        "    print(f\"   - Validation Hadith texts: {len(val_hadith_texts):,}\")\n",
        "    print(f\"   - Total validation texts: {len(val_ayah_texts) + len(val_hadith_texts):,}\")\n",
        "    print(f\"   - Validation examples (3x): {(len(val_ayah_texts) + len(val_hadith_texts)) * 3:,}\")\n",
        "\n",
        "    # Create training sets (remove validation texts from training)\n",
        "    val_ayah_set = set(val_ayah_texts)\n",
        "    val_hadith_set = set(val_hadith_texts)\n",
        "\n",
        "    train_ayah_texts = [text for text in ayah_texts if text not in val_ayah_set]\n",
        "    train_hadith_texts = [text for text in hadith_texts if text not in val_hadith_set]\n",
        "\n",
        "    print(f\"ðŸ“Š Training data after removing validation:\")\n",
        "    print(f\"   - Training Ayah texts: {len(train_ayah_texts):,}\")\n",
        "    print(f\"   - Training Hadith texts: {len(train_hadith_texts):,}\")\n",
        "    print(f\"   - Training examples (3x): {(len(train_ayah_texts) + len(train_hadith_texts)) * 3:,}\")\n",
        "    # --- END OF MODIFIED VALIDATION SPLIT ---\n",
        "\n",
        "    quran_train_prefixes = [\"\", \"Ù‚Ø§Ù„ Ø§Ù„Ù„Ù‡ ØªØ¹Ø§Ù„Ù‰:\", \"ÙˆÙ‚Ø§Ù„ Ø§Ù„Ù„Ù‡ Ø¹Ø² ÙˆØ¬Ù„:\", \"ÙƒÙ…Ø§ ÙˆØ±Ø¯ ÙÙŠ Ø§Ù„Ù‚Ø±Ø¢Ù† Ø§Ù„ÙƒØ±ÙŠÙ…:\", \"ÙˆÙÙŠ ÙƒØªØ§Ø¨ Ø§Ù„Ù„Ù‡:\", \"ÙˆÙ…Ù† Ø¢ÙŠØ§Øª Ø§Ù„Ù„Ù‡:\", \"ÙŠÙ‚ÙˆÙ„ Ø³Ø¨Ø­Ø§Ù†Ù‡ ÙˆØªØ¹Ø§Ù„Ù‰:\", \"ÙˆÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ø´Ø£Ù† ÙŠÙ‚ÙˆÙ„ Ø§Ù„Ù„Ù‡:\"]\n",
        "    quran_train_suffixes = [\"\", \"ØµØ¯Ù‚ Ø§Ù„Ù„Ù‡ Ø§Ù„Ø¹Ø¸ÙŠÙ…\", \"Ø¢ÙŠØ© ÙƒØ±ÙŠÙ…Ø©\", \"Ù…Ù† Ø§Ù„Ù‚Ø±Ø¢Ù† Ø§Ù„ÙƒØ±ÙŠÙ…\", \"ÙƒÙ„Ø§Ù… Ø§Ù„Ù„Ù‡ Ø¹Ø² ÙˆØ¬Ù„\", \"Ù…Ù† Ø§Ù„Ø°ÙƒØ± Ø§Ù„Ø­ÙƒÙŠÙ…\", \"ÙˆÙ„Ø°Ù„Ùƒ Ø¹Ø¨Ø±Ø© Ù„Ù„Ù…Ø¹ØªØ¨Ø±ÙŠÙ†\", \"ÙˆÙ‡Ø°Ø§ Ø¨ÙŠØ§Ù† Ù„Ù„Ù†Ø§Ø³\"]\n",
        "    hadith_train_prefixes = [\"\", \"Ù‚Ø§Ù„ Ø±Ø³ÙˆÙ„ Ø§Ù„Ù„Ù‡ ØµÙ„Ù‰ Ø§Ù„Ù„Ù‡ Ø¹Ù„ÙŠÙ‡ ÙˆØ³Ù„Ù…:\", \"ÙˆÙ‚Ø§Ù„ Ø§Ù„Ù†Ø¨ÙŠ ØµÙ„Ù‰ Ø§Ù„Ù„Ù‡ Ø¹Ù„ÙŠÙ‡ ÙˆØ³Ù„Ù…:\", \"Ø¹Ù† Ø§Ù„Ù†Ø¨ÙŠ ØµÙ„Ù‰ Ø§Ù„Ù„Ù‡ Ø¹Ù„ÙŠÙ‡ ÙˆØ³Ù„Ù…:\", \"Ø±ÙˆÙ‰ Ø£Ù† Ø§Ù„Ù†Ø¨ÙŠ ØµÙ„Ù‰ Ø§Ù„Ù„Ù‡ Ø¹Ù„ÙŠÙ‡ ÙˆØ³Ù„Ù… Ù‚Ø§Ù„:\", \"ÙˆÙÙŠ Ø§Ù„Ø­Ø¯ÙŠØ« Ø§Ù„Ø´Ø±ÙŠÙ:\", \"ÙˆØ¹Ù† Ø£Ø¨ÙŠ Ù‡Ø±ÙŠØ±Ø© Ø±Ø¶ÙŠ Ø§Ù„Ù„Ù‡ Ø¹Ù†Ù‡ Ù‚Ø§Ù„:\"]\n",
        "    hadith_train_suffixes = [\"\", \"Ø±ÙˆØ§Ù‡ Ø§Ù„Ø¨Ø®Ø§Ø±ÙŠ\", \"Ø±ÙˆØ§Ù‡ Ù…Ø³Ù„Ù…\", \"Ø­Ø¯ÙŠØ« ØµØ­ÙŠØ­\", \"ØµÙ„Ù‰ Ø§Ù„Ù„Ù‡ Ø¹Ù„ÙŠÙ‡ ÙˆØ³Ù„Ù…\", \"Ù…Ù† Ø§Ù„Ø³Ù†Ø© Ø§Ù„Ù†Ø¨ÙˆÙŠØ©\", \"(Ù…ØªÙÙ‚ Ø¹Ù„ÙŠÙ‡)\", \"Ø£Ùˆ ÙƒÙ…Ø§ Ù‚Ø§Ù„ ØµÙ„Ù‰ Ø§Ù„Ù„Ù‡ Ø¹Ù„ÙŠÙ‡ ÙˆØ³Ù„Ù…\"]\n",
        "    neutral_sentences = [\"ÙˆØ¨Ù†Ø§Ø¡ Ø¹Ù„Ù‰ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø£Ù† Ù†Ø³ØªÙ†ØªØ¬.\", \"ÙˆÙ‡Ø°Ø§ ÙŠÙˆØ¶Ø­ Ø¹Ø¸Ù…Ø© Ø§Ù„ØªØ´Ø±ÙŠØ¹.\", \"ÙˆÙÙŠ Ù‡Ø°Ø§ Ù‡Ø¯Ø§ÙŠØ© Ù„Ù„Ù…Ø¤Ù…Ù†ÙŠÙ†.\", \"Ø¥Ù† ÙÙŠ Ø°Ù„Ùƒ Ù„Ø¢ÙŠØ§Øª Ù„Ù‚ÙˆÙ… ÙŠØ¹Ù‚Ù„ÙˆÙ†.\", \"ÙˆÙ‡Ø°Ø§ Ù‡Ùˆ Ø§Ù„Ù‚ÙˆÙ„ Ø§Ù„Ø±Ø§Ø¬Ø­.\"]\n",
        "\n",
        "\n",
        "    print(\"ðŸ”„ Preprocessing training examples...\")\n",
        "    train_examples = []\n",
        "    ayah_csv_data, hadith_csv_data = [], []\n",
        "    failed_examples = 0\n",
        "\n",
        "    for ayah in tqdm(train_ayah_texts, desc=\"Training Ayahs\"):\n",
        "        for variation in range(3):\n",
        "            example = _create_example_fixed(ayah, 'Ayah', tokenizer, label_to_id, quran_train_prefixes, quran_train_suffixes, neutral_sentences, save_details=True)\n",
        "            if example:\n",
        "                train_examples.append({k: v for k, v in example.items() if k in [\"input_ids\", \"attention_mask\", \"labels\"]})\n",
        "                details = {k: v for k, v in example.items() if k not in [\"input_ids\", \"attention_mask\", \"labels\"]}\n",
        "                details.update({\"variation_number\": variation + 1, \"dataset_split\": \"training\"})\n",
        "                ayah_csv_data.append(details)\n",
        "            else:\n",
        "                failed_examples += 1\n",
        "\n",
        "    for hadith in tqdm(train_hadith_texts, desc=\"Training Hadiths\"):\n",
        "        for variation in range(3):\n",
        "            example = _create_example_fixed(hadith, 'Hadith', tokenizer, label_to_id, hadith_train_prefixes, hadith_train_suffixes, neutral_sentences, save_details=True)\n",
        "            if example:\n",
        "                train_examples.append({k: v for k, v in example.items() if k in [\"input_ids\", \"attention_mask\", \"labels\"]})\n",
        "                details = {k: v for k, v in example.items() if k not in [\"input_ids\", \"attention_mask\", \"labels\"]}\n",
        "                details.update({\"variation_number\": variation + 1, \"dataset_split\": \"training\"})\n",
        "                hadith_csv_data.append(details)\n",
        "            else:\n",
        "                failed_examples += 1\n",
        "\n",
        "    print(f\"âœ… Generated {len(train_examples)} training examples\")\n",
        "    print(f\"âŒ Failed to create {failed_examples} examples\")\n",
        "\n",
        "    validation_examples, validation_csv_data = create_validation_examples(tokenizer, label_to_id, val_ayah_texts, val_hadith_texts)\n",
        "\n",
        "    print(\"ðŸ’¾ Saving preprocessing details to CSV files...\")\n",
        "    pd.DataFrame(ayah_csv_data).to_csv(os.path.join(CSV_OUTPUT_DIR, \"ayah_training_details.csv\"), index=False, encoding='utf-8')\n",
        "    pd.DataFrame(hadith_csv_data).to_csv(os.path.join(CSV_OUTPUT_DIR, \"hadith_training_details.csv\"), index=False, encoding='utf-8')\n",
        "    pd.DataFrame(validation_csv_data).to_csv(os.path.join(CSV_OUTPUT_DIR, \"validation_details.csv\"), index=False, encoding='utf-8')\n",
        "    print(\"âœ… CSV files saved.\")\n",
        "\n",
        "    print(\"ðŸ’¾ Saving final tokenized datasets...\")\n",
        "    train_dataset = Dataset.from_list(train_examples)\n",
        "    val_dataset = Dataset.from_list(validation_examples)\n",
        "    train_dataset.save_to_disk(PREPROCESSED_TRAIN_PATH)\n",
        "    val_dataset.save_to_disk(PREPROCESSED_VAL_PATH)\n",
        "    print(f\"âœ… Datasets saved to {PREPROCESSED_TRAIN_PATH} and {PREPROCESSED_VAL_PATH}\")\n",
        "\n",
        "    # Updated summary with balanced validation info\n",
        "    summary_data = [\n",
        "        {\"dataset\": \"Training_Ayah\", \"total_examples\": len(ayah_csv_data), \"unique_texts\": len(train_ayah_texts)},\n",
        "        {\"dataset\": \"Training_Hadith\", \"total_examples\": len(hadith_csv_data), \"unique_texts\": len(train_hadith_texts)},\n",
        "        {\"dataset\": \"Validation_Ayah\", \"total_examples\": len(val_ayah_texts) * 3, \"unique_texts\": len(val_ayah_texts)},\n",
        "        {\"dataset\": \"Validation_Hadith\", \"total_examples\": len(val_hadith_texts) * 3, \"unique_texts\": len(val_hadith_texts)},\n",
        "        {\"dataset\": \"TOTAL\", \"total_examples\": len(train_examples) + len(validation_examples), \"failed_examples\": failed_examples}\n",
        "    ]\n",
        "    summary_df = pd.DataFrame(summary_data)\n",
        "    summary_path = os.path.join(CSV_OUTPUT_DIR, \"preprocessing_summary_balanced_30p.csv\")\n",
        "    summary_df.to_csv(summary_path, index=False)\n",
        "    print(f\"âœ… Preprocessing summary saved to: {summary_path}\")\n",
        "\n",
        "    # Print final balanced statistics\n",
        "    print(\"\\nðŸŽ‰ 30% BALANCED PREPROCESSING COMPLETE!\")\n",
        "    print(\"ðŸ“Š FINAL DATASET STATISTICS:\")\n",
        "    print(f\"   Training:   {len(train_ayah_texts):,} Ayahs + {len(train_hadith_texts):,} Hadiths = {len(train_examples):,} examples\")\n",
        "    print(f\"   Validation: {len(val_ayah_texts):,} Ayahs + {len(val_hadith_texts):,} Hadiths = {len(validation_examples):,} examples\")\n",
        "    print(f\"   Validation balance: {len(val_ayah_texts)/(len(val_ayah_texts)+len(val_hadith_texts))*100:.1f}% Ayah, {len(val_hadith_texts)/(len(val_ayah_texts)+len(val_hadith_texts))*100:.1f}% Hadith\")\n",
        "    print(f\"   ðŸŽ¯ Validation set is ~{round((len(val_ayah_texts) + len(val_hadith_texts)) / total_texts * 100)}% of the total unique texts.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main_preprocessing()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 920,
          "referenced_widgets": [
            "8d2b826305df4819b5cc656b034e4e75",
            "d72929a84a0f47abbd1beb4aa28327c1",
            "a320786042934d4da2bd122869964c29",
            "c91d6542f0884720bddcecef5d0a57b1",
            "9f3a9ca99ad0474a8eba2a3fead3cc6b",
            "7d3af2a420ea40deb6ef95bee5459daf",
            "1485bf2444e74983946e6a1cd0f88cf3",
            "ba12136699194ab89987ebf079a556c4",
            "a9bcbf86f02d431f91f392f2813a730c",
            "0e3b659106d64ff0b2e0659a27f72da0",
            "a81325e8a6b548568496613873ea8002",
            "355b7ec18502497ab94aae741559a58a",
            "adc79127f918417a8c682c599bbf849d",
            "dd22920dd039456393b107ccd2c7fec5",
            "049b3d33d9cf46cd8b660cf55b737abe",
            "55ddd6da80ca4070b638e176592ba983",
            "a0f31630efff4c49aa586399a2538c19",
            "60adc7da881b4947a4195c28749424ad",
            "57c426587fb14b99977591d3f8902600",
            "99f2790449e447f493dfa2fab03842dc",
            "46464d0834ed47da810d22f4bad46478",
            "f754d112118a481f949e01f2610fd497"
          ]
        },
        "id": "0ZP2-ChXtLOL",
        "outputId": "d800cba5-736d-4bb1-8eda-0249df0790bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ”„ STEP 1: OFFLINE PREPROCESSING WITH 30% BALANCED VALIDATION\n",
            "============================================================\n",
            "Loading raw data...\n",
            "ðŸ”ª Splitting Ayah texts longer than 25 tokens...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Ayahs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6236/6236 [00:00<00:00, 8136.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Splitting complete. Original: 6236 texts, New total: 6910 texts. (674 texts were split).\n",
            "ðŸ”„ Normalizing Ayah texts for data augmentation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Normalizing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6910/6910 [00:00<00:00, 88537.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Normalization complete. Ayah count increased from 6910 to 13820.\n",
            "Filtered: 13820 Ayahs, 31317 Hadiths\n",
            "ðŸŽ¯ Creating 30% BALANCED validation split:\n",
            "   - Total available texts: 45,137\n",
            "   - Target validation size (30%): 13,542 texts (6771 per class)\n",
            "   - Target validation examples (x3): 40,626 examples\n",
            "   - Available Ayah texts: 13,820\n",
            "   - Available Hadith texts: 31,317\n",
            "âœ… 30% balanced validation split created:\n",
            "   - Validation Ayah texts: 6,771\n",
            "   - Validation Hadith texts: 6,771\n",
            "   - Total validation texts: 13,542\n",
            "   - Validation examples (3x): 40,626\n",
            "ðŸ“Š Training data after removing validation:\n",
            "   - Training Ayah texts: 6,874\n",
            "   - Training Hadith texts: 24,159\n",
            "   - Training examples (3x): 93,099\n",
            "ðŸ”„ Preprocessing training examples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Ayahs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6874/6874 [00:04<00:00, 1498.24it/s]\n",
            "Training Hadiths: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24159/24159 [00:30<00:00, 789.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Generated 93099 training examples\n",
            "âŒ Failed to create 0 examples\n",
            "ðŸ”„ Creating generalization-focused validation examples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Val Ayahs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6771/6771 [00:04<00:00, 1390.51it/s]\n",
            "Val Hadiths: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6771/6771 [00:08<00:00, 818.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Created 40626 validation examples.\n",
            "ðŸ’¾ Saving preprocessing details to CSV files...\n",
            "âœ… CSV files saved.\n",
            "ðŸ’¾ Saving final tokenized datasets...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Saving the dataset (0/1 shards):   0%|          | 0/93099 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8d2b826305df4819b5cc656b034e4e75"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Saving the dataset (0/1 shards):   0%|          | 0/40626 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "355b7ec18502497ab94aae741559a58a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Datasets saved to /content/drive/MyDrive/FinalIslamic/prepros/preprocessed_train_30p_dataset and /content/drive/MyDrive/FinalIslamic/prepros/preprocessed_val_30p_dataset\n",
            "âœ… Preprocessing summary saved to: /content/drive/MyDrive/FinalIslamic/preprocessed_csv_30p/preprocessing_summary_balanced_30p.csv\n",
            "\n",
            "ðŸŽ‰ 30% BALANCED PREPROCESSING COMPLETE!\n",
            "ðŸ“Š FINAL DATASET STATISTICS:\n",
            "   Training:   6,874 Ayahs + 24,159 Hadiths = 93,099 examples\n",
            "   Validation: 6,771 Ayahs + 6,771 Hadiths = 40,626 examples\n",
            "   Validation balance: 50.0% Ayah, 50.0% Hadith\n",
            "   ðŸŽ¯ Validation set is ~30% of the total unique texts.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# SCRIPT 2: finetuning.py\n",
        "#\n",
        "# Purpose: Load pre-tokenized datasets from disk, fine-tune the AraBERT model,\n",
        "#          and generate a submission file for the test data.\n",
        "#\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForTokenClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForTokenClassification,\n",
        "    EarlyStoppingCallback\n",
        ")\n",
        "from datasets import load_from_disk\n",
        "import re\n",
        "import os\n",
        "import zipfile\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- 1. Configuration ---\n",
        "MODEL_NAME = \"aubmindlab/bert-base-arabertv2\"\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/FinalIslamic/arabert_finetuned_basline_30s\"\n",
        "TEST_XML_PATH = \"test_SubtaskA.xml\"\n",
        "\n",
        "# Paths for data prepared by preprocessing.py\n",
        "PREPROCESSED_TRAIN_PATH = \"/content/drive/MyDrive/FinalIslamic/prepros/preprocessed_train_30p_dataset\"\n",
        "PREPROCESSED_VAL_PATH = \"/content/drive/MyDrive/FinalIslamic/prepros/preprocessed_val_30p_dataset\"\n",
        "\n",
        "# Output paths for submission\n",
        "SUBMISSION_TSV_PATH = \"/content/drive/MyDrive/FinalIslamic/test/submission.tsv\"\n",
        "SUBMISSION_ZIP_PATH = \"/content/drive/MyDrive/FinalIslamic/test/submission.zip\"\n",
        "\n",
        "def fast_train_model():\n",
        "    \"\"\"Fast training using preprocessed data from disk.\"\"\"\n",
        "    print(\"ðŸš€ STEP 2: FAST TRAINING\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    if not os.path.exists(PREPROCESSED_TRAIN_PATH) or not os.path.exists(PREPROCESSED_VAL_PATH):\n",
        "        print(f\"âŒ Preprocessed datasets not found at {PREPROCESSED_TRAIN_PATH}\")\n",
        "        print(\"Please run the 'preprocessing.py' script first.\")\n",
        "        return False\n",
        "\n",
        "    print(\"ðŸ“¥ Loading preprocessed datasets...\")\n",
        "    train_dataset = load_from_disk(PREPROCESSED_TRAIN_PATH)\n",
        "    val_dataset = load_from_disk(PREPROCESSED_VAL_PATH)\n",
        "    print(f\"âœ… Loaded {len(train_dataset)} training and {len(val_dataset)} validation examples.\")\n",
        "\n",
        "    label_list = ['O', 'B-Ayah', 'I-Ayah', 'B-Hadith', 'I-Hadith']\n",
        "    id_to_label = {i: l for i, l in enumerate(label_list)}\n",
        "    label_to_id = {l: i for i, l in enumerate(label_list)}\n",
        "\n",
        "    model = AutoModelForTokenClassification.from_pretrained(\n",
        "        MODEL_NAME, num_labels=len(label_list), id2label=id_to_label, label2id=label_to_id\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=OUTPUT_DIR,\n",
        "        num_train_epochs=6,\n",
        "        per_device_train_batch_size=16,\n",
        "        gradient_accumulation_steps=2,\n",
        "        learning_rate=3e-5,\n",
        "        fp16=True,\n",
        "        logging_steps=100,\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"eval_loss\",\n",
        "        greater_is_better=False,\n",
        "        save_total_limit=2,\n",
        "        report_to=\"none\",\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=DataCollatorForTokenClassification(tokenizer),\n",
        "        callbacks=[EarlyStoppingCallback(early_stopping_patience=4)]\n",
        "    )\n",
        "\n",
        "    print(\"ðŸƒâ€â™‚ï¸ Starting training...\")\n",
        "    start_time = time.time()\n",
        "    trainer.train()\n",
        "    print(f\"â±ï¸ Training completed in {(time.time() - start_time)/60:.1f} minutes\")\n",
        "\n",
        "    trainer.save_model(OUTPUT_DIR)\n",
        "    print(f\"âœ… Best model saved to {OUTPUT_DIR}\")\n",
        "    return True\n",
        "\n",
        "def load_test_data_from_xml(xml_path):\n",
        "    \"\"\"Loads test data from the provided XML file format.\"\"\"\n",
        "    if not os.path.exists(xml_path):\n",
        "        print(f\"âŒ Test file not found at {xml_path}\")\n",
        "        return []\n",
        "    with open(xml_path, 'r', encoding='utf-8') as f:\n",
        "        content = f.read()\n",
        "    pattern = re.compile(r\"<Question>.*?<ID>(.*?)</ID>.*?<Response>(.*?)</Response>.*?</Question>\", re.DOTALL)\n",
        "    matches = pattern.findall(content)\n",
        "    return [{'Question_ID': m[0].strip(), 'Text': m[1].strip()} for m in matches]\n",
        "\n",
        "def predict_on_test_data(model, tokenizer, test_data):\n",
        "    \"\"\"Predicts spans on the test data.\"\"\"\n",
        "    print(\"ðŸ”® Predicting spans on test set...\")\n",
        "    model.eval()\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model.to(device)\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    label_list = list(model.config.id2label.values())\n",
        "    all_predictions = []\n",
        "\n",
        "    for item in tqdm(test_data, desc=\"Predicting\"):\n",
        "        qid, text = item[\"Question_ID\"], item[\"Text\"]\n",
        "        if not text.strip():\n",
        "            all_predictions.append({\"Question_ID\": qid, \"Span_Start\": 0, \"Span_End\": 0, \"Span_Type\": \"No_Spans\"})\n",
        "            continue\n",
        "\n",
        "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
        "        with torch.no_grad():\n",
        "            logits = model(**inputs).logits\n",
        "\n",
        "        preds = torch.argmax(logits, dim=2)[0].cpu().numpy()\n",
        "        spans = []\n",
        "        current_span = None\n",
        "        for i, pred_id in enumerate(preds):\n",
        "            label = label_list[pred_id]\n",
        "            word_id = inputs.word_ids(batch_index=0)[i]\n",
        "            if word_id is None: continue\n",
        "\n",
        "            if label.startswith('B-'):\n",
        "                if current_span: spans.append(current_span)\n",
        "                cs = inputs.token_to_chars(i)\n",
        "                current_span = {'type': label[2:], 'start': cs.start, 'end': cs.end}\n",
        "            elif label.startswith('I-') and current_span and current_span['type'] == label[2:]:\n",
        "                cs = inputs.token_to_chars(i)\n",
        "                current_span['end'] = cs.end\n",
        "            elif current_span:\n",
        "                spans.append(current_span)\n",
        "                current_span = None\n",
        "        if current_span:\n",
        "            spans.append(current_span)\n",
        "\n",
        "        if spans:\n",
        "            for span in spans:\n",
        "                all_predictions.append({\"Question_ID\": qid, \"Span_Start\": span['start'], \"Span_End\": span['end'], \"Span_Type\": span['type']})\n",
        "        else:\n",
        "            all_predictions.append({\"Question_ID\": qid, \"Span_Start\": 0, \"Span_End\": 0, \"Span_Type\": \"No_Spans\"})\n",
        "\n",
        "    return all_predictions\n",
        "\n",
        "def generate_submission_file(predictions, output_path, zip_path):\n",
        "    \"\"\"Generates the final submission.tsv and submission.zip files.\"\"\"\n",
        "    print(f\"ðŸ“¦ Generating submission file at {output_path}...\")\n",
        "    df = pd.DataFrame(predictions)[[\"Question_ID\", \"Span_Start\", \"Span_End\", \"Span_Type\"]]\n",
        "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "    df.to_csv(output_path, sep='\\t', index=False, header=False)\n",
        "    with zipfile.ZipFile(zip_path, 'w') as zf:\n",
        "        zf.write(output_path, os.path.basename(output_path))\n",
        "    print(f\"âœ… Submission zip created successfully at {zip_path}\")\n",
        "\n",
        "\n",
        "def main_finetuning():\n",
        "    \"\"\"Main function to run training and prediction.\"\"\"\n",
        "    # --- Training ---\n",
        "    if not os.path.exists(OUTPUT_DIR):\n",
        "        if not fast_train_model():\n",
        "            print(\"âŒ Training failed. Exiting.\")\n",
        "            return\n",
        "    else:\n",
        "        print(f\"âœ… Found existing fine-tuned model at {OUTPUT_DIR}. Skipping training.\")\n",
        "\n",
        "    # --- Prediction ---\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"ðŸš€ STEP 3: PREDICTION\")\n",
        "    print(\"=\"*50)\n",
        "    model = AutoModelForTokenClassification.from_pretrained(OUTPUT_DIR)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(OUTPUT_DIR)\n",
        "\n",
        "    test_data = load_test_data_from_xml(TEST_XML_PATH)\n",
        "    if test_data:\n",
        "        predictions = predict_on_test_data(model, tokenizer, test_data)\n",
        "        generate_submission_file(predictions, SUBMISSION_TSV_PATH, SUBMISSION_ZIP_PATH)\n",
        "        print(\"\\nðŸŽ‰ Full pipeline completed successfully!\")\n",
        "    else:\n",
        "        print(\"âŒ Could not load test data. Prediction step skipped.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main_finetuning()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 605,
          "referenced_widgets": [
            "58b6c73a7f1746beb1f5dc080a4261f1",
            "7c8efaa094844de2a0f7e4d172b4a3b8",
            "b12e589a721e443b931ec33b31b1517c",
            "ef56dd3d7dfb4689bbf0fa2d9eb7cc53",
            "7068950b4fa84c6899e9b47efb2fcfc9",
            "2f9c088f18f14a3985d66a16bc9d060e",
            "1c0355ea78354151bf298baeef176c20",
            "137fdb617bb54723931ccdf6a500746b",
            "0674d681a7b640b1b8db3d8b8738c0dc",
            "46826a12c2cf4a719d261f7f0c9d47e7",
            "2345fec650f744eda864758488e3ea1d"
          ]
        },
        "id": "BTBaT4cBtLQi",
        "outputId": "ed15b806-dbf9-4602-c7f3-4422e8ca0218"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸš€ STEP 2: FAST TRAINING\n",
            "==================================================\n",
            "ðŸ“¥ Loading preprocessed datasets...\n",
            "âœ… Loaded 93099 training and 40626 validation examples.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/543M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "58b6c73a7f1746beb1f5dc080a4261f1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at aubmindlab/bert-base-arabertv2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipython-input-2299629343.py:80: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸƒâ€â™‚ï¸ Starting training...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='17460' max='17460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [17460/17460 36:31, Epoch 6/6]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.000400</td>\n",
              "      <td>0.007366</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.000500</td>\n",
              "      <td>0.004823</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.001100</td>\n",
              "      <td>0.031810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.000500</td>\n",
              "      <td>0.028795</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.000300</td>\n",
              "      <td>0.017632</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.000500</td>\n",
              "      <td>0.023038</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â±ï¸ Training completed in 36.6 minutes\n",
            "âœ… Best model saved to /content/drive/MyDrive/FinalIslamic/arabert_finetuned_basline_30s\n",
            "\n",
            "==================================================\n",
            "ðŸš€ STEP 3: PREDICTION\n",
            "==================================================\n",
            "âŒ Test file not found at test_SubtaskA.xml\n",
            "âŒ Could not load test data. Prediction step skipped.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# SCRIPT: evaluate_finetuned_model.py\n",
        "#\n",
        "# Purpose: Evaluate the performance of a fine-tuned transformer model on the development set\n",
        "#          using the official character-level scoring logic.\n",
        "#\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "import re\n",
        "import os\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- Configuration ---\n",
        "# Path to your fine-tuned model directory\n",
        "FINETUNED_MODEL_PATH = \"/content/drive/MyDrive/FinalIslamic/arabert_finetuned_basline_30s\"\n",
        "\n",
        "# Paths to the development set files\n",
        "DEV_XML_PATH = \"/content/drive/MyDrive/FinalIslamic/data/dev_SubtaskA.xml\"\n",
        "DEV_TSV_PATH = \"/content/drive/MyDrive/FinalIslamic/data/dev_SubtaskA.tsv\"\n",
        "\n",
        "def load_dev_data_from_xml(xml_path):\n",
        "    \"\"\"Loads development data from XML file.\"\"\"\n",
        "    print(f\"ðŸ“– Loading development text data from {xml_path}...\")\n",
        "    try:\n",
        "        with open(xml_path, 'r', encoding='utf-8') as f:\n",
        "            content = f.read()\n",
        "        pattern = re.compile(r\"<Question>.*?<ID>(.*?)</ID>.*?<Response>(.*?)</Response>.*?</Question>\", re.DOTALL)\n",
        "        matches = pattern.findall(content)\n",
        "        dev_texts = {m[0].strip(): m[1].strip() for m in matches}\n",
        "        print(f\"âœ… Successfully loaded {len(dev_texts)} development questions\")\n",
        "        return dev_texts\n",
        "    except FileNotFoundError:\n",
        "        print(f\"âŒ Error: XML file not found at {xml_path}\")\n",
        "        return {}\n",
        "\n",
        "def predict_with_finetuned_model(model, tokenizer, dev_texts_dict):\n",
        "    \"\"\"Generates predictions using the fine-tuned transformer model.\"\"\"\n",
        "    print(\"ðŸ¤– Predicting spans using fine-tuned model...\")\n",
        "    model.eval()\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model.to(device)\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    label_list = list(model.config.id2label.values())\n",
        "    all_predictions = []\n",
        "\n",
        "    for question_id, text in tqdm(dev_texts_dict.items(), desc=\"Predicting on Dev Set\"):\n",
        "        if not text or not text.strip():\n",
        "            all_predictions.append({\"Question_ID\": question_id, \"Span_Start\": 0, \"Span_End\": 0, \"Span_Type\": \"No_Spans\"})\n",
        "            continue\n",
        "\n",
        "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
        "        with torch.no_grad():\n",
        "            logits = model(**inputs).logits\n",
        "\n",
        "        preds = torch.argmax(logits, dim=2)[0].cpu().numpy()\n",
        "\n",
        "        spans = []\n",
        "        current_span = None\n",
        "        for i, pred_id in enumerate(preds):\n",
        "            label = label_list[pred_id]\n",
        "            word_id = inputs.word_ids(batch_index=0)[i]\n",
        "\n",
        "            if word_id is None:  # Skip special tokens\n",
        "                continue\n",
        "\n",
        "            if label.startswith('B-'):\n",
        "                if current_span:  # Close previous span if it exists\n",
        "                    spans.append(current_span)\n",
        "\n",
        "                # Start a new span\n",
        "                char_span = inputs.token_to_chars(i)\n",
        "                current_span = {'type': label[2:], 'start': char_span.start, 'end': char_span.end}\n",
        "\n",
        "            elif label.startswith('I-') and current_span and current_span['type'] == label[2:]:\n",
        "                # Extend the current span\n",
        "                char_span = inputs.token_to_chars(i)\n",
        "                current_span['end'] = char_span.end\n",
        "\n",
        "            else:  # 'O' label or mismatched 'I-' label\n",
        "                if current_span:\n",
        "                    spans.append(current_span)\n",
        "                current_span = None\n",
        "\n",
        "        if current_span:  # Add the last span if the text ends with it\n",
        "            spans.append(current_span)\n",
        "\n",
        "        if spans:\n",
        "            for span in spans:\n",
        "                all_predictions.append({\n",
        "                    \"Question_ID\": question_id,\n",
        "                    \"Span_Start\": span['start'],\n",
        "                    \"Span_End\": span['end'],\n",
        "                    \"Span_Type\": span['type']\n",
        "                })\n",
        "        else:\n",
        "            all_predictions.append({\n",
        "                \"Question_ID\": question_id,\n",
        "                \"Span_Start\": question_id,\n",
        "                \"Span_End\": 0,\n",
        "                \"Span_Type\": \"No_Spans\"\n",
        "            })\n",
        "\n",
        "    return pd.DataFrame(all_predictions)\n",
        "\n",
        "def evaluate_using_scoring_logic(predictions_df, reference_df, qid_response_mapping):\n",
        "    \"\"\"Evaluates predictions using the same logic as the official scoring script.\"\"\"\n",
        "    print(\"\\nðŸŽ¯ Starting Evaluation using official scoring logic...\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    Normal_Text_Tag, Ayah_Tag, Hadith_Tag = 0, 1, 2\n",
        "    all_y_true, all_y_pred = [], []\n",
        "\n",
        "    span_stats = {\n",
        "        'total_questions': 0, 'questions_with_annotations': 0, 'questions_with_predictions': 0,\n",
        "        'no_annotation_questions': 0, 'correct_no_spans': 0, 'total_true_spans': 0,\n",
        "        'total_pred_spans': 0, 'per_question_f1': []\n",
        "    }\n",
        "\n",
        "    total_f1, count_valid_question = 0, 0\n",
        "\n",
        "    for question_id in reference_df['Question_ID'].unique():\n",
        "        span_stats['total_questions'] += 1\n",
        "\n",
        "        if question_id not in predictions_df['Question_ID'].values or question_id not in qid_response_mapping:\n",
        "            continue\n",
        "\n",
        "        count_valid_question += 1\n",
        "        question_result = reference_df[reference_df['Question_ID'] == question_id]\n",
        "\n",
        "        if len(question_result) > 0 and question_result['Label'].values[0] == 'NoAnnotation':\n",
        "            span_stats['no_annotation_questions'] += 1\n",
        "            pred_spans = predictions_df[predictions_df['Question_ID'] == question_id]\n",
        "            if len(pred_spans) > 0 and pred_spans['Span_Type'].values[0] == 'No_Spans':\n",
        "                total_f1 += 1.0\n",
        "                span_stats['correct_no_spans'] += 1\n",
        "                span_stats['per_question_f1'].append(1.0)\n",
        "            else:\n",
        "                span_stats['per_question_f1'].append(0.0)\n",
        "            continue\n",
        "\n",
        "        span_stats['questions_with_annotations'] += 1\n",
        "        response_text = qid_response_mapping[question_id]\n",
        "\n",
        "        # Create prediction character array\n",
        "        pred_char_array = [Normal_Text_Tag] * len(response_text)\n",
        "        pred_result = predictions_df[predictions_df['Question_ID'] == question_id]\n",
        "\n",
        "        pred_span_count = 0\n",
        "        if len(pred_result) > 0 and pred_result['Span_Type'].values[0] != 'No_Spans':\n",
        "            span_stats['questions_with_predictions'] += 1\n",
        "            for _, row in pred_result.iterrows():\n",
        "                pred_span_count += 1\n",
        "                start, end, type = int(row['Span_Start']), int(row['Span_End']), row['Span_Type']\n",
        "                if start >= 0 and end <= len(response_text):\n",
        "                    tag = Ayah_Tag if type == 'Ayah' else Hadith_Tag\n",
        "                    pred_char_array[start:end] = [tag] * (end - start)\n",
        "        span_stats['total_pred_spans'] += pred_span_count\n",
        "\n",
        "        # Create truth character array\n",
        "        truth_char_array = [Normal_Text_Tag] * len(response_text)\n",
        "        true_span_count = 0\n",
        "        for _, row in question_result.iterrows():\n",
        "            true_span_count += 1\n",
        "            start, end, type = int(row['Span_Start']), int(row['Span_End']), row['Label']\n",
        "            if end <= len(response_text) and start >= 0:\n",
        "                tag = Ayah_Tag if type == 'Ayah' else Hadith_Tag\n",
        "                truth_char_array[start:end] = [tag] * (end - start)\n",
        "        span_stats['total_true_spans'] += true_span_count\n",
        "\n",
        "\n",
        "        f1 = f1_score(truth_char_array, pred_char_array, average='macro', zero_division=0)\n",
        "        total_f1 += f1\n",
        "        span_stats['per_question_f1'].append(f1)\n",
        "\n",
        "\n",
        "        all_y_true.extend(truth_char_array)\n",
        "        all_y_pred.extend(pred_char_array)\n",
        "\n",
        "    f1_score_value = total_f1 / count_valid_question if count_valid_question > 0 else 0.0\n",
        "    generate_comprehensive_stats(all_y_true, all_y_pred, span_stats, f1_score_value)\n",
        "    return f1_score_value\n",
        "\n",
        "\n",
        "def generate_comprehensive_stats(y_true, y_pred, span_stats, final_f1):\n",
        "    \"\"\"Generates and prints the detailed EDA and evaluation statistics.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"ðŸ“Š COMPREHENSIVE EVALUATION STATISTICS (EDA)\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    label_map = {0: 'Neither', 1: 'Ayah', 2: 'Hadith'}\n",
        "    y_true_labels = [label_map[label] for label in y_true]\n",
        "    y_pred_labels = [label_map[label] for label in y_pred]\n",
        "\n",
        "    print(f\"\\nðŸ“ˆ CHARACTER-LEVEL CLASSIFICATION REPORT\")\n",
        "    print(\"-\" * 60)\n",
        "    labels = ['Neither', 'Ayah', 'Hadith']\n",
        "    print(classification_report(y_true_labels, y_pred_labels, labels=labels, zero_division=0, digits=4))\n",
        "\n",
        "    print(f\"\\nðŸ“‹ SPAN-LEVEL STATISTICS\")\n",
        "    print(\"-\" * 60)\n",
        "    print(f\"Total questions processed: {span_stats['total_questions']}\")\n",
        "    print(f\"Questions with ground truth annotations: {span_stats['questions_with_annotations']}\")\n",
        "    print(f\"'No annotation' questions: {span_stats['no_annotation_questions']}\")\n",
        "    print(f\"Correct 'No_Spans' predictions: {span_stats['correct_no_spans']}/{span_stats['no_annotation_questions']}\")\n",
        "    print(f\"Span counts (True vs. Predicted): {span_stats['total_true_spans']} vs. {span_stats['total_pred_spans']}\")\n",
        "\n",
        "    if span_stats['per_question_f1']:\n",
        "        per_q_f1 = np.array(span_stats['per_question_f1'])\n",
        "        print(f\"\\nPer-question F1 statistics:\")\n",
        "        print(f\"  Mean F1: {np.mean(per_q_f1):.4f} | Median F1: {np.median(per_q_f1):.4f} | Std Dev: {np.std(per_q_f1):.4f}\")\n",
        "        print(f\"  Questions with perfect F1 (1.0): {np.sum(per_q_f1 == 1.0)}\")\n",
        "        print(f\"  Questions with zero F1 (0.0): {np.sum(per_q_f1 == 0.0)}\")\n",
        "\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"ðŸŽ¯ FINAL SUMMARY\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"**Final Macro-Averaged F1 Score: {final_f1:.4f}**\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "# --- Main Execution ---\n",
        "def main():\n",
        "    print(\"ðŸ” Fine-Tuned Model Evaluation on Development Set\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Load model and tokenizer\n",
        "    print(f\"ðŸš€ Loading fine-tuned model from: {FINETUNED_MODEL_PATH}\")\n",
        "    if not os.path.exists(FINETUNED_MODEL_PATH):\n",
        "        print(\"âŒ Model directory not found. Please ensure the path is correct.\")\n",
        "        return\n",
        "\n",
        "    model = AutoModelForTokenClassification.from_pretrained(FINETUNED_MODEL_PATH)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(FINETUNED_MODEL_PATH)\n",
        "\n",
        "    # Load development data\n",
        "    dev_texts_dict = load_dev_data_from_xml(DEV_XML_PATH)\n",
        "    if not dev_texts_dict:\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        ground_truth_df = pd.read_csv(DEV_TSV_PATH, sep='\\t')\n",
        "        print(f\"âœ… Successfully loaded {len(ground_truth_df)} ground truth annotations\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"âŒ Error: Ground truth file not found at {DEV_TSV_PATH}\")\n",
        "        return\n",
        "\n",
        "    # Generate predictions\n",
        "    predictions_df = predict_with_finetuned_model(model, tokenizer, dev_texts_dict)\n",
        "\n",
        "    # Evaluate predictions\n",
        "    final_f1 = evaluate_using_scoring_logic(predictions_df, ground_truth_df, dev_texts_dict)\n",
        "\n",
        "    print(f\"\\nðŸŽ‰ EVALUATION COMPLETED!\")\n",
        "    print(f\"ðŸŽ¯ Final Macro F1-Score on the development set: {final_f1:.4f}\")\n",
        "\n",
        "    # Save dev predictions for inspection\n",
        "    output_path = '/content/finetuned_model_dev_predictions.tsv'\n",
        "    predictions_df.to_csv(output_path, sep='\\t', index=False, header=True)\n",
        "    print(f\"ðŸ“ Development set predictions saved to: {output_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6M41HL_67vh",
        "outputId": "2bc70b40-f0e1-4d53-a081-c79cd32e3331"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ” Fine-Tuned Model Evaluation on Development Set\n",
            "============================================================\n",
            "ðŸš€ Loading fine-tuned model from: /content/drive/MyDrive/FinalIslamic/arabert_finetuned_basline_30s\n",
            "ðŸ“– Loading development text data from /content/drive/MyDrive/FinalIslamic/data/dev_SubtaskA.xml...\n",
            "âœ… Successfully loaded 50 development questions\n",
            "âœ… Successfully loaded 210 ground truth annotations\n",
            "ðŸ¤– Predicting spans using fine-tuned model...\n",
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Predicting on Dev Set: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:00<00:00, 70.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸŽ¯ Starting Evaluation using official scoring logic...\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "ðŸ“Š COMPREHENSIVE EVALUATION STATISTICS (EDA)\n",
            "============================================================\n",
            "\n",
            "ðŸ“ˆ CHARACTER-LEVEL CLASSIFICATION REPORT\n",
            "------------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Neither     0.8423    0.9688    0.9011     42711\n",
            "        Ayah     0.8326    0.5574    0.6678     12381\n",
            "      Hadith     0.4750    0.3333    0.3917      7798\n",
            "\n",
            "    accuracy                         0.8090     62890\n",
            "   macro avg     0.7166    0.6198    0.6535     62890\n",
            "weighted avg     0.7948    0.8090    0.7920     62890\n",
            "\n",
            "\n",
            "ðŸ“‹ SPAN-LEVEL STATISTICS\n",
            "------------------------------------------------------------\n",
            "Total questions processed: 50\n",
            "Questions with ground truth annotations: 34\n",
            "'No annotation' questions: 16\n",
            "Correct 'No_Spans' predictions: 13/16\n",
            "Span counts (True vs. Predicted): 194 vs. 149\n",
            "\n",
            "Per-question F1 statistics:\n",
            "  Mean F1: 0.6508 | Median F1: 0.6326 | Std Dev: 0.3066\n",
            "  Questions with perfect F1 (1.0): 13\n",
            "  Questions with zero F1 (0.0): 3\n",
            "\n",
            "============================================================\n",
            "ðŸŽ¯ FINAL SUMMARY\n",
            "============================================================\n",
            "**Final Macro-Averaged F1 Score: 0.6508**\n",
            "============================================================\n",
            "\n",
            "ðŸŽ‰ EVALUATION COMPLETED!\n",
            "ðŸŽ¯ Final Macro F1-Score on the development set: 0.6508\n",
            "ðŸ“ Development set predictions saved to: /content/finetuned_model_dev_predictions.tsv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L6qgPtTztLUJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}