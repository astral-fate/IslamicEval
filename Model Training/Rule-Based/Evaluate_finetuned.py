#
#Â SCRIPT:Â evaluate_finetuned_model.py
#
#Â Purpose:Â EvaluateÂ theÂ performanceÂ ofÂ aÂ fine-tunedÂ transformerÂ modelÂ onÂ theÂ developmentÂ set
#Â Â Â Â Â Â Â Â Â Â usingÂ theÂ officialÂ character-levelÂ scoringÂ logic.
#

importÂ pandasÂ asÂ pd
importÂ numpyÂ asÂ np
importÂ torch
fromÂ transformersÂ importÂ AutoTokenizer,Â AutoModelForTokenClassification
importÂ re
importÂ os
fromÂ sklearn.metricsÂ importÂ f1_score,Â classification_report
fromÂ tqdmÂ importÂ tqdm

#Â ---Â ConfigurationÂ ---
#Â PathÂ toÂ yourÂ fine-tunedÂ modelÂ directory
FINETUNED_MODEL_PATHÂ =Â "/content/drive/MyDrive/FinalIslamic/arabert_finetuned_model_best_v2"

#Â PathsÂ toÂ theÂ developmentÂ setÂ files
DEV_XML_PATHÂ =Â "/content/drive/MyDrive/FinalIslamic/data/dev_SubtaskA.xml"
DEV_TSV_PATHÂ =Â "/content/drive/MyDrive/FinalIslamic/data/dev_SubtaskA.tsv"

defÂ load_dev_data_from_xml(xml_path):
Â Â Â Â """LoadsÂ developmentÂ dataÂ fromÂ XMLÂ file."""
Â Â Â Â print(f"ğŸ“–Â LoadingÂ developmentÂ textÂ dataÂ fromÂ {xml_path}...")
Â Â Â Â try:
Â Â Â Â Â Â Â Â withÂ open(xml_path,Â 'r',Â encoding='utf-8')Â asÂ f:
Â Â Â Â Â Â Â Â Â Â Â Â contentÂ =Â f.read()
Â Â Â Â Â Â Â Â patternÂ =Â re.compile(r"<Question>.*?<ID>(.*?)</ID>.*?<Response>(.*?)</Response>.*?</Question>",Â re.DOTALL)
Â Â Â Â Â Â Â Â matchesÂ =Â pattern.findall(content)
Â Â Â Â Â Â Â Â dev_textsÂ =Â {m[0].strip():Â m[1].strip()Â forÂ mÂ inÂ matches}
Â Â Â Â Â Â Â Â print(f"âœ…Â SuccessfullyÂ loadedÂ {len(dev_texts)}Â developmentÂ questions")
Â Â Â Â Â Â Â Â returnÂ dev_texts
Â Â Â Â exceptÂ FileNotFoundError:
Â Â Â Â Â Â Â Â print(f"âŒÂ Error:Â XMLÂ fileÂ notÂ foundÂ atÂ {xml_path}")
Â Â Â Â Â Â Â Â returnÂ {}

defÂ predict_with_finetuned_model(model,Â tokenizer,Â dev_texts_dict):
Â Â Â Â """GeneratesÂ predictionsÂ usingÂ theÂ fine-tunedÂ transformerÂ model."""
Â Â Â Â print("ğŸ¤–Â PredictingÂ spansÂ usingÂ fine-tunedÂ model...")
Â Â Â Â model.eval()
Â Â Â Â deviceÂ =Â "cuda"Â ifÂ torch.cuda.is_available()Â elseÂ "cpu"
Â Â Â Â model.to(device)
Â Â Â Â print(f"UsingÂ device:Â {device}")

Â Â Â Â label_listÂ =Â list(model.config.id2label.values())
Â Â Â Â all_predictionsÂ =Â []

Â Â Â Â forÂ question_id,Â textÂ inÂ tqdm(dev_texts_dict.items(),Â desc="PredictingÂ onÂ DevÂ Set"):
Â Â Â Â Â Â Â Â ifÂ notÂ textÂ orÂ notÂ text.strip():
Â Â Â Â Â Â Â Â Â Â Â Â all_predictions.append({"Question_ID":Â question_id,Â "Span_Start":Â 0,Â "Span_End":Â 0,Â "Span_Type":Â "No_Spans"})
Â Â Â Â Â Â Â Â Â Â Â Â continue

Â Â Â Â Â Â Â Â inputsÂ =Â tokenizer(text,Â return_tensors="pt",Â truncation=True,Â max_length=512).to(device)
Â Â Â Â Â Â Â Â withÂ torch.no_grad():
Â Â Â Â Â Â Â Â Â Â Â Â logitsÂ =Â model(**inputs).logits

Â Â Â Â Â Â Â Â predsÂ =Â torch.argmax(logits,Â dim=2)[0].cpu().numpy()

Â Â Â Â Â Â Â Â spansÂ =Â []
Â Â Â Â Â Â Â Â current_spanÂ =Â None
Â Â Â Â Â Â Â Â forÂ i,Â pred_idÂ inÂ enumerate(preds):
Â Â Â Â Â Â Â Â Â Â Â Â labelÂ =Â label_list[pred_id]
Â Â Â Â Â Â Â Â Â Â Â Â word_idÂ =Â inputs.word_ids(batch_index=0)[i]

Â Â Â Â Â Â Â Â Â Â Â Â ifÂ word_idÂ isÂ None:Â Â #Â SkipÂ specialÂ tokens
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â continue

Â Â Â Â Â Â Â Â Â Â Â Â ifÂ label.startswith('B-'):
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â ifÂ current_span:Â Â #Â CloseÂ previousÂ spanÂ ifÂ itÂ exists
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â spans.append(current_span)

Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â #Â StartÂ aÂ newÂ span
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â char_spanÂ =Â inputs.token_to_chars(i)
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â current_spanÂ =Â {'type':Â label[2:],Â 'start':Â char_span.start,Â 'end':Â char_span.end}

Â Â Â Â Â Â Â Â Â Â Â Â elifÂ label.startswith('I-')Â andÂ current_spanÂ andÂ current_span['type']Â ==Â label[2:]:
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â #Â ExtendÂ theÂ currentÂ span
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â char_spanÂ =Â inputs.token_to_chars(i)
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â current_span['end']Â =Â char_span.end

Â Â Â Â Â Â Â Â Â Â Â Â else:Â Â #Â 'O'Â labelÂ orÂ mismatchedÂ 'I-'Â label
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â ifÂ current_span:
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â spans.append(current_span)
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â current_spanÂ =Â None

Â Â Â Â Â Â Â Â ifÂ current_span:Â Â #Â AddÂ theÂ lastÂ spanÂ ifÂ theÂ textÂ endsÂ withÂ it
Â Â Â Â Â Â Â Â Â Â Â Â spans.append(current_span)

Â Â Â Â Â Â Â Â ifÂ spans:
Â Â Â Â Â Â Â Â Â Â Â Â forÂ spanÂ inÂ spans:
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â all_predictions.append({
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â "Question_ID":Â question_id,
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â "Span_Start":Â span['start'],
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â "Span_End":Â span['end'],
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â "Span_Type":Â span['type']
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â })
Â Â Â Â Â Â Â Â else:
Â Â Â Â Â Â Â Â Â Â Â Â all_predictions.append({
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â "Question_ID":Â question_id,
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â "Span_Start":Â 0,
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â "Span_End":Â 0,
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â "Span_Type":Â "No_Spans"
Â Â Â Â Â Â Â Â Â Â Â Â })

Â Â Â Â returnÂ pd.DataFrame(all_predictions)

defÂ evaluate_using_scoring_logic(predictions_df,Â reference_df,Â qid_response_mapping):
Â Â Â Â """EvaluatesÂ predictionsÂ usingÂ theÂ sameÂ logicÂ asÂ theÂ officialÂ scoringÂ script."""
Â Â Â Â print("\nğŸ¯Â StartingÂ EvaluationÂ usingÂ officialÂ scoringÂ logic...")
Â Â Â Â print("="Â *Â 60)

Â Â Â Â Normal_Text_Tag,Â Ayah_Tag,Â Hadith_TagÂ =Â 0,Â 1,Â 2
Â Â Â Â all_y_true,Â all_y_predÂ =Â [],Â []

Â Â Â Â span_statsÂ =Â {
Â Â Â Â Â Â Â Â 'total_questions':Â 0,Â 'questions_with_annotations':Â 0,Â 'questions_with_predictions':Â 0,
Â Â Â Â Â Â Â Â 'no_annotation_questions':Â 0,Â 'correct_no_spans':Â 0,Â 'total_true_spans':Â 0,
Â Â Â Â Â Â Â Â 'total_pred_spans':Â 0,Â 'per_question_f1':Â []
Â Â Â Â }

Â Â Â Â total_f1,Â count_valid_questionÂ =Â 0,Â 0

Â Â Â Â forÂ question_idÂ inÂ reference_df['Question_ID'].unique():
Â Â Â Â Â Â Â Â span_stats['total_questions']Â +=Â 1

Â Â Â Â Â Â Â Â ifÂ question_idÂ notÂ inÂ predictions_df['Question_ID'].valuesÂ orÂ question_idÂ notÂ inÂ qid_response_mapping:
Â Â Â Â Â Â Â Â Â Â Â Â continue

Â Â Â Â Â Â Â Â count_valid_questionÂ +=Â 1
Â Â Â Â Â Â Â Â question_resultÂ =Â reference_df[reference_df['Question_ID']Â ==Â question_id]

Â Â Â Â Â Â Â Â ifÂ len(question_result)Â >Â 0Â andÂ question_result['Label'].values[0]Â ==Â 'NoAnnotation':
Â Â Â Â Â Â Â Â Â Â Â Â span_stats['no_annotation_questions']Â +=Â 1
Â Â Â Â Â Â Â Â Â Â Â Â pred_spansÂ =Â predictions_df[predictions_df['Question_ID']Â ==Â question_id]
Â Â Â Â Â Â Â Â Â Â Â Â ifÂ len(pred_spans)Â >Â 0Â andÂ pred_spans['Span_Type'].values[0]Â ==Â 'No_Spans':
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â total_f1Â +=Â 1.0
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â span_stats['correct_no_spans']Â +=Â 1
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â span_stats['per_question_f1'].append(1.0)
Â Â Â Â Â Â Â Â Â Â Â Â else:
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â span_stats['per_question_f1'].append(0.0)
Â Â Â Â Â Â Â Â Â Â Â Â continue

Â Â Â Â Â Â Â Â span_stats['questions_with_annotations']Â +=Â 1
Â Â Â Â Â Â Â Â response_textÂ =Â qid_response_mapping[question_id]

Â Â Â Â Â Â Â Â #Â CreateÂ predictionÂ characterÂ array
Â Â Â Â Â Â Â Â pred_char_arrayÂ =Â [Normal_Text_Tag]Â *Â len(response_text)
Â Â Â Â Â Â Â Â pred_resultÂ =Â predictions_df[predictions_df['Question_ID']Â ==Â question_id]

Â Â Â Â Â Â Â Â pred_span_countÂ =Â 0
Â Â Â Â Â Â Â Â ifÂ len(pred_result)Â >Â 0Â andÂ pred_result['Span_Type'].values[0]Â !=Â 'No_Spans':
Â Â Â Â Â Â Â Â Â Â Â Â span_stats['questions_with_predictions']Â +=Â 1
Â Â Â Â Â Â Â Â Â Â Â Â forÂ _,Â rowÂ inÂ pred_result.iterrows():
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â pred_span_countÂ +=Â 1
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â start,Â end,Â typeÂ =Â int(row['Span_Start']),Â int(row['Span_End']),Â row['Span_Type']
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â ifÂ startÂ >=Â 0Â andÂ endÂ <=Â len(response_text):
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â tagÂ =Â Ayah_TagÂ ifÂ typeÂ ==Â 'Ayah'Â elseÂ Hadith_Tag
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â pred_char_array[start:end]Â =Â [tag]Â *Â (endÂ -Â start)
Â Â Â Â Â Â Â Â span_stats['total_pred_spans']Â +=Â pred_span_count

Â Â Â Â Â Â Â Â #Â CreateÂ truthÂ characterÂ array
Â Â Â Â Â Â Â Â truth_char_arrayÂ =Â [Normal_Text_Tag]Â *Â len(response_text)
Â Â Â Â Â Â Â Â true_span_countÂ =Â 0
Â Â Â Â Â Â Â Â forÂ _,Â rowÂ inÂ question_result.iterrows():
Â Â Â Â Â Â Â Â Â Â Â Â true_span_countÂ +=Â 1
Â Â Â Â Â Â Â Â Â Â Â Â start,Â end,Â typeÂ =Â int(row['Span_Start']),Â int(row['Span_End']),Â row['Label']
Â Â Â Â Â Â Â Â Â Â Â Â ifÂ endÂ <=Â len(response_text)Â andÂ startÂ >=Â 0:
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â tagÂ =Â Ayah_TagÂ ifÂ typeÂ ==Â 'Ayah'Â elseÂ Hadith_Tag
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â truth_char_array[start:end]Â =Â [tag]Â *Â (endÂ -Â start)
Â Â Â Â Â Â Â Â span_stats['total_true_spans']Â +=Â true_span_count

Â Â Â Â Â Â Â Â f1Â =Â f1_score(truth_char_array,Â pred_char_array,Â average='macro',Â zero_division=0)
Â Â Â Â Â Â Â Â total_f1Â +=Â f1
Â Â Â Â Â Â Â Â span_stats['per_question_f1'].append(f1)

Â Â Â Â Â Â Â Â all_y_true.extend(truth_char_array)
Â Â Â Â Â Â Â Â all_y_pred.extend(pred_char_array)

Â Â Â Â f1_score_valueÂ =Â total_f1Â /Â count_valid_questionÂ ifÂ count_valid_questionÂ >Â 0Â elseÂ 0.0
Â Â Â Â generate_comprehensive_stats(all_y_true,Â all_y_pred,Â span_stats,Â f1_score_value)
Â Â Â Â returnÂ f1_score_value

defÂ generate_comprehensive_stats(y_true,Â y_pred,Â span_stats,Â final_f1):
Â Â Â Â """GeneratesÂ andÂ printsÂ theÂ detailedÂ EDAÂ andÂ evaluationÂ statistics."""
Â Â Â Â print("\n"Â +Â "="*60)
Â Â Â Â print("ğŸ“ŠÂ COMPREHENSIVEÂ EVALUATIONÂ STATISTICSÂ (EDA)")
Â Â Â Â print("="*60)

Â Â Â Â label_mapÂ =Â {0:Â 'Neither',Â 1:Â 'Ayah',Â 2:Â 'Hadith'}
Â Â Â Â y_true_labelsÂ =Â [label_map[label]Â forÂ labelÂ inÂ y_true]
Â Â Â Â y_pred_labelsÂ =Â [label_map[label]Â forÂ labelÂ inÂ y_pred]

Â Â Â Â print(f"\nğŸ“ˆÂ CHARACTER-LEVELÂ CLASSIFICATIONÂ REPORT")
Â Â Â Â print("-"Â *Â 60)
Â Â Â Â labelsÂ =Â ['Neither',Â 'Ayah',Â 'Hadith']
Â Â Â Â print(classification_report(y_true_labels,Â y_pred_labels,Â labels=labels,Â zero_division=0,Â digits=4))

Â Â Â Â print(f"\nğŸ“‹Â SPAN-LEVELÂ STATISTICS")
Â Â Â Â print("-"Â *Â 60)
Â Â Â Â print(f"TotalÂ questionsÂ processed:Â {span_stats['total_questions']}")
Â Â Â Â print(f"QuestionsÂ withÂ groundÂ truthÂ annotations:Â {span_stats['questions_with_annotations']}")
Â Â Â Â print(f"'NoÂ annotation'Â questions:Â {span_stats['no_annotation_questions']}")
Â Â Â Â print(f"CorrectÂ 'No_Spans'Â predictions:Â {span_stats['correct_no_spans']}/{span_stats['no_annotation_questions']}")
Â Â Â Â print(f"SpanÂ countsÂ (TrueÂ vs.Â Predicted):Â {span_stats['total_true_spans']}Â vs.Â {span_stats['total_pred_spans']}")

Â Â Â Â ifÂ span_stats['per_question_f1']:
Â Â Â Â Â Â Â Â per_q_f1Â =Â np.array(span_stats['per_question_f1'])
Â Â Â Â Â Â Â Â print(f"\nPer-questionÂ F1Â statistics:")
Â Â Â Â Â Â Â Â print(f"Â Â MeanÂ F1:Â {np.mean(per_q_f1):.4f}Â |Â MedianÂ F1:Â {np.median(per_q_f1):.4f}Â |Â StdÂ Dev:Â {np.std(per_q_f1):.4f}")
Â Â Â Â Â Â Â Â print(f"Â Â QuestionsÂ withÂ perfectÂ F1Â (1.0):Â {np.sum(per_q_f1Â ==Â 1.0)}")
Â Â Â Â Â Â Â Â print(f"Â Â QuestionsÂ withÂ zeroÂ F1Â (0.0):Â {np.sum(per_q_f1Â ==Â 0.0)}")

Â Â Â Â print("\n"Â +Â "="*60)
Â Â Â Â print("ğŸ¯Â FINALÂ SUMMARY")
Â Â Â Â print("="*60)
Â Â Â Â print(f"**FinalÂ Macro-AveragedÂ F1Â Score:Â {final_f1:.4f}**")
Â Â Â Â print("="*60)

#Â ---Â MainÂ ExecutionÂ ---
defÂ main():
Â Â Â Â print("ğŸ”Â Fine-TunedÂ ModelÂ EvaluationÂ onÂ DevelopmentÂ Set")
Â Â Â Â print("="Â *Â 60)

Â Â Â Â #Â LoadÂ modelÂ andÂ tokenizer
Â Â Â Â print(f"ğŸš€Â LoadingÂ fine-tunedÂ modelÂ from:Â {FINETUNED_MODEL_PATH}")
Â Â Â Â ifÂ notÂ os.path.exists(FINETUNED_MODEL_PATH):
Â Â Â Â Â Â Â Â print("âŒÂ ModelÂ directoryÂ notÂ found.Â PleaseÂ ensureÂ theÂ pathÂ isÂ correct.")
Â Â Â Â Â Â Â Â return

Â Â Â Â modelÂ =Â AutoModelForTokenClassification.from_pretrained(FINETUNED_MODEL_PATH)
Â Â Â Â tokenizerÂ =Â AutoTokenizer.from_pretrained(FINETUNED_MODEL_PATH)

Â Â Â Â #Â LoadÂ developmentÂ data
Â Â Â Â dev_texts_dictÂ =Â load_dev_data_from_xml(DEV_XML_PATH)
Â Â Â Â ifÂ notÂ dev_texts_dict:
Â Â Â Â Â Â Â Â return

Â Â Â Â try:
Â Â Â Â Â Â Â Â ground_truth_dfÂ =Â pd.read_csv(DEV_TSV_PATH,Â sep='\t')
Â Â Â Â Â Â Â Â print(f"âœ…Â SuccessfullyÂ loadedÂ {len(ground_truth_df)}Â groundÂ truthÂ annotations")
Â Â Â Â exceptÂ FileNotFoundError:
Â Â Â Â Â Â Â Â print(f"âŒÂ Error:Â GroundÂ truthÂ fileÂ notÂ foundÂ atÂ {DEV_TSV_PATH}")
Â Â Â Â Â Â Â Â return

Â Â Â Â #Â GenerateÂ predictions
Â Â Â Â predictions_dfÂ =Â predict_with_finetuned_model(model,Â tokenizer,Â dev_texts_dict)

Â Â Â Â #Â EvaluateÂ predictions
Â Â Â Â final_f1Â =Â evaluate_using_scoring_logic(predictions_df,Â ground_truth_df,Â dev_texts_dict)

Â Â Â Â print(f"\nğŸ‰Â EVALUATIONÂ COMPLETED!")
Â Â Â Â print(f"ğŸ¯Â FinalÂ MacroÂ F1-ScoreÂ onÂ theÂ developmentÂ set:Â {final_f1:.4f}")

Â Â Â Â #Â SaveÂ devÂ predictionsÂ forÂ inspection
Â Â Â Â output_pathÂ =Â '/content/finetuned_model_dev_predictions.tsv'
Â Â Â Â predictions_df.to_csv(output_path,Â sep='\t',Â index=False,Â header=True)
Â Â Â Â print(f"ğŸ“Â DevelopmentÂ setÂ predictionsÂ savedÂ to:Â {output_path}")

ifÂ __name__Â ==Â "__main__":
Â  Â  main()
