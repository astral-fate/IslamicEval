#
#Â SCRIPTÂ 1:Â preprocessing.pyÂ (MAXIMUMÂ COMPATIBILITYÂ VERSION)
#
#Â Purpose:Â LoadÂ rawÂ QuranÂ andÂ HadithÂ data,Â strategicallyÂ splitÂ longÂ Ayahs,
#Â Â Â Â Â Â Â Â Â Â aggressivelyÂ cleanÂ allÂ textsÂ toÂ ensureÂ maximumÂ inclusion,Â addÂ explicitÂ quotes,
#Â Â Â Â Â Â Â Â Â Â andÂ saveÂ theÂ finalÂ tokenizedÂ datasetsÂ toÂ disk.
#

importÂ json
importÂ pandasÂ asÂ pd
fromÂ datasetsÂ importÂ Dataset
importÂ re
importÂ os
importÂ random
fromÂ tqdmÂ importÂ tqdm
fromÂ transformersÂ importÂ AutoTokenizer
importÂ nltk

#Â ---Â 1.Â ConfigurationÂ ---
MODEL_NAMEÂ =Â "aubmindlab/bert-base-arabertv2"

#Â InputÂ dataÂ paths
QURAN_JSON_PATHÂ =Â "/content/drive/MyDrive/FinalIslamic/data/quran.json"
SIX_HADITH_BOOKS_JSON_PATHÂ =Â "/content/drive/MyDrive/FinalIslamic/data/six_hadith_books.json"

#Â OutputÂ pathsÂ forÂ processedÂ data
PREPROCESSED_TRAIN_PATHÂ =Â "/content/drive/MyDrive/FinalIslamic/prepros/preprocessed_train_dataset"
PREPROCESSED_VAL_PATHÂ =Â "/content/drive/MyDrive/FinalIslamic/prepros/preprocessed_val_dataset"
CSV_OUTPUT_DIRÂ =Â "/content/drive/MyDrive/FinalIslamic/preprocessed_csv/"

defÂ split_long_texts(texts,Â tokenizer,Â max_tokens=80,Â label_type="Ayah"):
Â Â Â Â """
Â Â Â Â SplitsÂ longÂ textsÂ intoÂ smallerÂ chunksÂ basedÂ onÂ tokenÂ length.
Â Â Â Â ItÂ triesÂ toÂ findÂ aÂ naturalÂ splitÂ pointÂ (likeÂ aÂ periodÂ orÂ space)Â nearÂ theÂ middle.
Â Â Â Â """
Â Â Â Â print(f"ğŸ”ªÂ SplittingÂ {label_type}Â textsÂ longerÂ thanÂ {max_tokens}Â tokens...")
Â Â Â Â split_textsÂ =Â []
Â Â Â Â split_countÂ =Â 0
Â Â Â Â forÂ textÂ inÂ tqdm(texts,Â desc=f"ProcessingÂ {label_type}s"):
Â Â Â Â Â Â Â Â tokensÂ =Â tokenizer.tokenize(text)
Â Â Â Â Â Â Â Â ifÂ len(tokens)Â <=Â max_tokens:
Â Â Â Â Â Â Â Â Â Â Â Â split_texts.append(text)
Â Â Â Â Â Â Â Â else:
Â Â Â Â Â Â Â Â Â Â Â Â mid_pointÂ =Â len(text)Â //Â 2
Â Â Â Â Â Â Â Â Â Â Â Â split_posÂ =Â text.rfind('à¥¤',Â 0,Â mid_point)
Â Â Â Â Â Â Â Â Â Â Â Â ifÂ split_posÂ ==Â -1:
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â split_posÂ =Â text.rfind('Â ',Â 0,Â mid_point)
Â Â Â Â Â Â Â Â Â Â Â Â ifÂ split_posÂ ==Â -1:
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â split_posÂ =Â mid_point

Â Â Â Â Â Â Â Â Â Â Â Â part1Â =Â text[:split_pos].strip()
Â Â Â Â Â Â Â Â Â Â Â Â part2Â =Â text[split_pos:].strip()

Â Â Â Â Â Â Â Â Â Â Â Â ifÂ part1:Â split_texts.append(part1)
Â Â Â Â Â Â Â Â Â Â Â Â ifÂ part2:Â split_texts.append(part2)
Â Â Â Â Â Â Â Â Â Â Â Â split_countÂ +=Â 1

Â Â Â Â print(f"âœ…Â SplittingÂ complete.Â Original:Â {len(texts)}Â texts,Â NewÂ total:Â {len(split_texts)}Â texts.Â ({split_count}Â textsÂ wereÂ split).")
Â Â Â Â returnÂ split_texts


defÂ _create_example_fixed(text,Â label_type,Â tokenizer,Â label_to_id,Â prefixes,Â suffixes,Â neutral_sentences,Â save_details=False):
Â Â Â Â """CreatesÂ aÂ singleÂ tokenizedÂ exampleÂ withÂ context."""
Â Â Â Â try:
Â Â Â Â Â Â Â Â #Â ---Â CHANGE:Â CleanÂ theÂ textÂ FIRSTÂ toÂ preventÂ find()Â mismatchesÂ ---
Â Â Â Â Â Â Â Â cleaned_textÂ =Â re.sub(r'\s+',Â 'Â ',Â text).strip()

Â Â Â Â Â Â Â Â ifÂ notÂ cleaned_text:Â #Â FilterÂ onlyÂ ifÂ theÂ textÂ isÂ completelyÂ emptyÂ afterÂ cleaning
Â Â Â Â Â Â Â Â Â Â Â Â returnÂ None

Â Â Â Â Â Â Â Â prefixÂ =Â random.choice(prefixes)
Â Â Â Â Â Â Â Â suffixÂ =Â random.choice(suffixes)

Â Â Â Â Â Â Â Â ifÂ random.random()Â >Â 0.3:
Â Â Â Â Â Â Â Â Â Â Â Â contextÂ =Â random.choice(neutral_sentences)
Â Â Â Â Â Â Â Â Â Â Â Â ifÂ random.random()Â >Â 0.5:
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â full_textÂ =Â f'{prefix}Â {context}Â "{cleaned_text}"Â {suffix}'
Â Â Â Â Â Â Â Â Â Â Â Â else:
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â full_textÂ =Â f'{prefix}Â "{cleaned_text}"Â {context}Â {suffix}'
Â Â Â Â Â Â Â Â else:
Â Â Â Â Â Â Â Â Â Â Â Â full_textÂ =Â f'{prefix}Â "{cleaned_text}"Â {suffix}'

Â Â Â Â Â Â Â Â #Â CleanÂ theÂ finalÂ combinedÂ textÂ asÂ well
Â Â Â Â Â Â Â Â full_textÂ =Â re.sub(r'\s+',Â 'Â ',Â full_text).strip()

Â Â Â Â Â Â Â Â #Â Now,Â findingÂ theÂ cleaned_textÂ withinÂ full_textÂ isÂ guaranteedÂ toÂ work
Â Â Â Â Â Â Â Â char_startÂ =Â full_text.find(cleaned_text)
Â Â Â Â Â Â Â Â ifÂ char_startÂ ==Â -1:
Â Â Â Â Â Â Â Â Â Â Â Â #Â ThisÂ shouldÂ almostÂ neverÂ happenÂ now
Â Â Â Â Â Â Â Â Â Â Â Â returnÂ None
Â Â Â Â Â Â Â Â char_endÂ =Â char_startÂ +Â len(cleaned_text)

Â Â Â Â Â Â Â Â tokenized_inputÂ =Â tokenizer(full_text,Â truncation=True,Â max_length=512)
Â Â Â Â Â Â Â Â input_idsÂ =Â tokenized_input['input_ids']
Â Â Â Â Â Â Â Â attention_maskÂ =Â tokenized_input['attention_mask']
Â Â Â Â Â Â Â Â labelsÂ =Â [label_to_id['O']]Â *Â len(input_ids)

Â Â Â Â Â Â Â Â start_tokenÂ =Â tokenized_input.char_to_token(char_start)
Â Â Â Â Â Â Â Â end_tokenÂ =Â tokenized_input.char_to_token(char_endÂ -Â 1)

Â Â Â Â Â Â Â Â ifÂ start_tokenÂ isÂ notÂ NoneÂ andÂ end_tokenÂ isÂ notÂ None:
Â Â Â Â Â Â Â Â Â Â Â Â labels[start_token]Â =Â label_to_id[f'B-{label_type}']
Â Â Â Â Â Â Â Â Â Â Â Â forÂ iÂ inÂ range(start_tokenÂ +Â 1,Â min(end_tokenÂ +Â 1,Â len(labels))):
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â labels[i]Â =Â label_to_id[f'I-{label_type}']

Â Â Â Â Â Â Â Â word_idsÂ =Â tokenized_input.word_ids()
Â Â Â Â Â Â Â Â final_labelsÂ =Â []
Â Â Â Â Â Â Â Â forÂ i,Â word_idÂ inÂ enumerate(word_ids):
Â Â Â Â Â Â Â Â Â Â Â Â ifÂ word_idÂ isÂ NoneÂ orÂ (iÂ >Â 0Â andÂ word_idÂ ==Â word_ids[iÂ -Â 1]):
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â final_labels.append(-100)
Â Â Â Â Â Â Â Â Â Â Â Â else:
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â final_labels.append(labels[i]Â ifÂ iÂ <Â len(labels)Â elseÂ label_to_id['O'])

Â Â Â Â Â Â Â Â resultÂ =Â {
Â Â Â Â Â Â Â Â Â Â Â Â "input_ids":Â input_ids,
Â Â Â Â Â Â Â Â Â Â Â Â "attention_mask":Â attention_mask,
Â Â Â Â Â Â Â Â Â Â Â Â "labels":Â final_labels
Â Â Â Â Â Â Â Â }

Â Â Â Â Â Â Â Â ifÂ save_details:
Â Â Â Â Â Â Â Â Â Â Â Â result.update({
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â "original_text":Â text,Â #Â LogÂ theÂ originalÂ uncleanedÂ text
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â "full_text":Â full_text,
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â "prefix":Â prefix,
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â "suffix":Â suffix,
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â "char_start":Â char_start,
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â "char_end":Â char_end,
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â "label_type":Â label_type,
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â "target_span":Â cleaned_textÂ #Â TheÂ targetÂ isÂ theÂ cleanedÂ version
Â Â Â Â Â Â Â Â Â Â Â Â })
Â Â Â Â Â Â Â Â returnÂ result
Â Â Â Â exceptÂ Exception:
Â Â Â Â Â Â Â Â returnÂ None

defÂ create_validation_examples(tokenizer,Â label_to_id,Â val_ayah_texts,Â val_hadith_texts):
Â Â Â Â """CreatesÂ validationÂ examplesÂ usingÂ aÂ differentÂ setÂ ofÂ patternsÂ toÂ testÂ generalization."""
Â Â Â Â print("ğŸ”„Â CreatingÂ generalization-focusedÂ validationÂ examples...")

Â Â Â Â val_ayah_prefixesÂ =Â ["",Â "ÙˆÙÙŠÂ Ø§Ù„Ù‚Ø±Ø¢Ù†Â Ø§Ù„ÙƒØ±ÙŠÙ…Â Ù†Ø¬Ø¯:",Â "ÙˆÙ…Ù†Â Ø¢ÙŠØ§ØªÂ Ø§Ù„Ù„Ù‡:",Â "ÙˆÙ‚Ø¯Â Ø£Ù†Ø²Ù„Â Ø§Ù„Ù„Ù‡:",Â "ÙˆÙŠÙ‚ÙˆÙ„Â Ø§Ù„Ø­Ù‚Â ØªØ¨Ø§Ø±ÙƒÂ ÙˆØªØ¹Ø§Ù„Ù‰:",Â "ÙˆÙÙŠÂ Ø§Ù„Ø°ÙƒØ±Â Ø§Ù„Ø­ÙƒÙŠÙ…:",Â "ÙˆÙÙŠÂ ÙƒØªØ§Ø¨Â Ø§Ù„Ù„Ù‡Â Ù†Ù‚Ø±Ø£:",Â "ÙˆØ§Ù„Ø¯Ù„ÙŠÙ„Â Ø¹Ù„Ù‰Â Ø°Ù„ÙƒÂ Ù‚ÙˆÙ„Ù‡Â ØªØ¹Ø§Ù„Ù‰:"]
Â Â Â Â val_ayah_suffixesÂ =Â ["",Â "Ù‡Ø°Ø§Â Ù…Ù†Â ÙƒÙ„Ø§Ù…Â Ø§Ù„Ù„Ù‡",Â "Ø¢ÙŠØ©Â Ø¹Ø¸ÙŠÙ…Ø©",Â "Ù…Ù†Â Ø§Ù„Ù‚Ø±Ø¢Ù†Â Ø§Ù„ÙƒØ±ÙŠÙ…",Â "ÙƒÙ„Ø§Ù…Â Ø±Ø¨Â Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠÙ†",Â "Ù…Ù†Â Ø§Ù„Ø°ÙƒØ±Â Ø§Ù„Ø­ÙƒÙŠÙ…",Â "Ø¢ÙŠØ©Â ÙƒØ±ÙŠÙ…Ø©",Â "(ØµØ¯Ù‚Â Ø§Ù„Ù„Ù‡Â Ø§Ù„Ø¹Ø¸ÙŠÙ…)"]
Â Â Â Â val_hadith_prefixesÂ =Â ["",Â "ÙˆÙÙŠÂ Ø§Ù„Ø³Ù†Ø©Â Ø§Ù„Ù†Ø¨ÙˆÙŠØ©:",Â "ÙˆÙ…Ù†Â Ù‡Ø¯ÙŠÂ Ø§Ù„Ù†Ø¨ÙŠÂ ØµÙ„Ù‰Â Ø§Ù„Ù„Ù‡Â Ø¹Ù„ÙŠÙ‡Â ÙˆØ³Ù„Ù…:",Â "ÙˆÙ‚Ø¯Â Ø¹Ù„Ù…Ù†Ø§Â Ø§Ù„Ø±Ø³ÙˆÙ„Â ØµÙ„Ù‰Â Ø§Ù„Ù„Ù‡Â Ø¹Ù„ÙŠÙ‡Â ÙˆØ³Ù„Ù…:",Â "ÙˆÙÙŠÂ Ø§Ù„Ø­Ø¯ÙŠØ«Â Ø§Ù„Ø´Ø±ÙŠÙÂ Ù†Ø¬Ø¯:",Â "ÙƒÙ…Ø§Â Ø¬Ø§Ø¡Â ÙÙŠÂ Ø§Ù„Ø­Ø¯ÙŠØ«:"]
Â Â Â Â val_hadith_suffixesÂ =Â ["",Â "Ù…Ù†Â Ø§Ù„Ø³Ù†Ø©Â Ø§Ù„Ù†Ø¨ÙˆÙŠØ©",Â "Ø­Ø¯ÙŠØ«Â Ù†Ø¨ÙˆÙŠÂ Ø´Ø±ÙŠÙ",Â "Ù…Ù†Â Ù‡Ø¯ÙŠÂ Ø§Ù„Ù…ØµØ·ÙÙ‰",Â "ØµÙ„Ù‰Â Ø§Ù„Ù„Ù‡Â Ø¹Ù„ÙŠÙ‡Â ÙˆØ³Ù„Ù…",Â "(Ø±ÙˆØ§Ù‡Â Ø§Ù„ØªØ±Ù…Ø°ÙŠ)"]
Â Â Â Â val_transitionsÂ =Â ["ÙˆÙ„Ù†ØªØ£Ù…Ù„Â Ù…Ø¹Ø§Ù‹",Â "ÙˆÙÙŠÂ Ù‡Ø°Ø§Â Ø§Ù„Ø³ÙŠØ§Ù‚",Â "ÙˆÙ„Ù„ØªÙˆØ¶ÙŠØ­",Â "ÙˆØ¥Ù„ÙŠÙƒÙ…Â Ø§Ù„Ù…Ø«Ø§Ù„",Â "ÙˆÙÙŠÂ Ù‡Ø°Ø§Â Ø§Ù„ØµØ¯Ø¯",Â "ÙˆÙ‡Ø°Ø§Â ÙŠØ¨ÙŠÙ†Â Ù„Ù†Ø§Â Ø£Ù‡Ù…ÙŠØ©Â Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹."]

Â Â Â Â validation_dataÂ =Â []
Â Â Â Â validation_csv_dataÂ =Â []

Â Â Â Â forÂ ayahÂ inÂ tqdm(val_ayah_texts,Â desc="ValÂ Ayahs"):
Â Â Â Â Â Â Â Â forÂ variation_numÂ inÂ range(3):
Â Â Â Â Â Â Â Â Â Â Â Â exampleÂ =Â _create_example_fixed(ayah,Â 'Ayah',Â tokenizer,Â label_to_id,Â val_ayah_prefixes,Â val_ayah_suffixes,Â val_transitions,Â save_details=True)
Â Â Â Â Â Â Â Â Â Â Â Â ifÂ example:
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â validation_data.append({k:Â vÂ forÂ k,Â vÂ inÂ example.items()Â ifÂ kÂ inÂ ["input_ids",Â "attention_mask",Â "labels"]})
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â detailsÂ =Â {k:Â vÂ forÂ k,Â vÂ inÂ example.items()Â ifÂ kÂ notÂ inÂ ["input_ids",Â "attention_mask",Â "labels"]}
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â details.update({"variation_number":Â variation_numÂ +Â 1,Â "dataset_split":Â "validation"})
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â validation_csv_data.append(details)

Â Â Â Â forÂ hadithÂ inÂ tqdm(val_hadith_texts,Â desc="ValÂ Hadiths"):
Â Â Â Â Â Â Â Â forÂ variation_numÂ inÂ range(3):
Â Â Â Â Â Â Â Â Â Â Â Â exampleÂ =Â _create_example_fixed(hadith,Â 'Hadith',Â tokenizer,Â label_to_id,Â val_hadith_prefixes,Â val_hadith_suffixes,Â val_transitions,Â save_details=True)
Â Â Â Â Â Â Â Â Â Â Â Â ifÂ example:
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â validation_data.append({k:Â vÂ forÂ k,Â vÂ inÂ example.items()Â ifÂ kÂ inÂ ["input_ids",Â "attention_mask",Â "labels"]})
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â detailsÂ =Â {k:Â vÂ forÂ k,Â vÂ inÂ example.items()Â ifÂ kÂ notÂ inÂ ["input_ids",Â "attention_mask",Â "labels"]}
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â details.update({"variation_number":Â variation_numÂ +Â 1,Â "dataset_split":Â "validation"})
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â validation_csv_data.append(details)

Â Â Â Â print(f"âœ…Â CreatedÂ {len(validation_data)}Â validationÂ examples.")
Â Â Â Â returnÂ validation_data,Â validation_csv_data


defÂ main_preprocessing():
Â Â Â Â """MainÂ functionÂ toÂ runÂ theÂ entireÂ preprocessingÂ pipeline."""
Â Â Â Â print("ğŸ”„Â STEPÂ 1:Â OFFLINEÂ PREPROCESSING")
Â Â Â Â print("="Â *Â 50)

Â Â Â Â os.makedirs(CSV_OUTPUT_DIR,Â exist_ok=True)
Â Â Â Â tokenizerÂ =Â AutoTokenizer.from_pretrained(MODEL_NAME)
Â Â Â Â label_listÂ =Â ['O',Â 'B-Ayah',Â 'I-Ayah',Â 'B-Hadith',Â 'I-Hadith']
Â Â Â Â label_to_idÂ =Â {l:Â iÂ forÂ i,Â lÂ inÂ enumerate(label_list)}

Â Â Â Â print("LoadingÂ rawÂ data...")
Â Â Â Â withÂ open(QURAN_JSON_PATH,Â 'r',Â encoding='utf-8')Â asÂ f:
Â Â Â Â Â Â Â Â quran_dataÂ =Â json.load(f)
Â Â Â Â withÂ open(SIX_HADITH_BOOKS_JSON_PATH,Â 'r',Â encoding='utf-8')Â asÂ f:
Â Â Â Â Â Â Â Â six_books_dataÂ =Â json.load(f)

Â Â Â Â ayah_textsÂ =Â [item['ayah_text']Â forÂ itemÂ inÂ quran_dataÂ ifÂ 'ayah_text'Â inÂ item]
Â Â Â Â hadith_textsÂ =Â [item['Matn'].strip()Â forÂ itemÂ inÂ six_books_dataÂ ifÂ 'Matn'Â inÂ itemÂ andÂ item['Matn']Â andÂ item['Matn'].strip()]

Â Â Â Â ayah_textsÂ =Â split_long_texts(ayah_texts,Â tokenizer,Â max_tokens=80,Â label_type="Ayah")

Â Â Â Â MAX_TEXT_LENGTHÂ =Â 1500
Â Â Â Â ayah_textsÂ =Â [tÂ forÂ tÂ inÂ ayah_textsÂ ifÂ len(t)Â <Â MAX_TEXT_LENGTH]
Â Â Â Â hadith_textsÂ =Â [tÂ forÂ tÂ inÂ hadith_textsÂ ifÂ len(t)Â <Â MAX_TEXT_LENGTH]
Â Â Â Â print(f"Filtered:Â {len(ayah_texts)}Â Ayahs,Â {len(hadith_texts)}Â Hadiths")

Â Â Â Â random.seed(42)
Â Â Â Â all_textsÂ =Â [(text,Â 'Ayah')Â forÂ textÂ inÂ ayah_texts]Â +Â [(text,Â 'Hadith')Â forÂ textÂ inÂ hadith_texts]
Â Â Â Â val_subset_sizeÂ =Â min(int(len(all_texts)Â *Â 0.20),Â 3333)
Â Â Â Â val_texts_subsetÂ =Â random.sample(all_texts,Â val_subset_size)
Â Â Â Â val_ayah_textsÂ =Â [textÂ forÂ text,Â labelÂ inÂ val_texts_subsetÂ ifÂ labelÂ ==Â 'Ayah']
Â Â Â Â val_hadith_textsÂ =Â [textÂ forÂ text,Â labelÂ inÂ val_texts_subsetÂ ifÂ labelÂ ==Â 'Hadith']

Â Â Â Â train_ayah_textsÂ =Â ayah_texts
Â Â Â Â train_hadith_textsÂ =Â hadith_texts

Â Â Â Â quran_train_prefixesÂ =Â ["",Â "Ù‚Ø§Ù„Â Ø§Ù„Ù„Ù‡Â ØªØ¹Ø§Ù„Ù‰:",Â "ÙˆÙ‚Ø§Ù„Â Ø§Ù„Ù„Ù‡Â Ø¹Ø²Â ÙˆØ¬Ù„:",Â "ÙƒÙ…Ø§Â ÙˆØ±Ø¯Â ÙÙŠÂ Ø§Ù„Ù‚Ø±Ø¢Ù†Â Ø§Ù„ÙƒØ±ÙŠÙ…:",Â "ÙˆÙÙŠÂ ÙƒØªØ§Ø¨Â Ø§Ù„Ù„Ù‡:",Â "ÙˆÙ…Ù†Â Ø¢ÙŠØ§ØªÂ Ø§Ù„Ù„Ù‡:",Â "ÙŠÙ‚ÙˆÙ„Â Ø³Ø¨Ø­Ø§Ù†Ù‡Â ÙˆØªØ¹Ø§Ù„Ù‰:",Â "ÙˆÙÙŠÂ Ù‡Ø°Ø§Â Ø§Ù„Ø´Ø£Ù†Â ÙŠÙ‚ÙˆÙ„Â Ø§Ù„Ù„Ù‡:"]
Â Â Â Â quran_train_suffixesÂ =Â ["",Â "ØµØ¯Ù‚Â Ø§Ù„Ù„Ù‡Â Ø§Ù„Ø¹Ø¸ÙŠÙ…",Â "Ø¢ÙŠØ©Â ÙƒØ±ÙŠÙ…Ø©",Â "Ù…Ù†Â Ø§Ù„Ù‚Ø±Ø¢Ù†Â Ø§Ù„ÙƒØ±ÙŠÙ…",Â "ÙƒÙ„Ø§Ù…Â Ø§Ù„Ù„Ù‡Â Ø¹Ø²Â ÙˆØ¬Ù„",Â "Ù…Ù†Â Ø§Ù„Ø°ÙƒØ±Â Ø§Ù„Ø­ÙƒÙŠÙ…",Â "(Ø³ÙˆØ±Ø©Â Ø§Ù„Ø¨Ù‚Ø±Ø©ØŒÂ Ø§Ù„Ø¢ÙŠØ©Â 255)",Â "ÙˆÙ‡Ø°Ø§Â Ø¨ÙŠØ§Ù†Â Ù„Ù„Ù†Ø§Ø³"]
Â Â Â Â hadith_train_prefixesÂ =Â ["",Â "Ù‚Ø§Ù„Â Ø±Ø³ÙˆÙ„Â Ø§Ù„Ù„Ù‡Â ØµÙ„Ù‰Â Ø§Ù„Ù„Ù‡Â Ø¹Ù„ÙŠÙ‡Â ÙˆØ³Ù„Ù…:",Â "ÙˆÙ‚Ø§Ù„Â Ø§Ù„Ù†Ø¨ÙŠÂ ØµÙ„Ù‰Â Ø§Ù„Ù„Ù‡Â Ø¹Ù„ÙŠÙ‡Â ÙˆØ³Ù„Ù…:",Â "Ø¹Ù†Â Ø§Ù„Ù†Ø¨ÙŠÂ ØµÙ„Ù‰Â Ø§Ù„Ù„Ù‡Â Ø¹Ù„ÙŠÙ‡Â ÙˆØ³Ù„Ù…:",Â "Ø±ÙˆÙ‰Â Ø£Ù†Â Ø§Ù„Ù†Ø¨ÙŠÂ ØµÙ„Ù‰Â Ø§Ù„Ù„Ù‡Â Ø¹Ù„ÙŠÙ‡Â ÙˆØ³Ù„Ù…Â Ù‚Ø§Ù„:",Â "ÙˆÙÙŠÂ Ø§Ù„Ø­Ø¯ÙŠØ«Â Ø§Ù„Ø´Ø±ÙŠÙ:",Â "ÙˆØ¹Ù†Â Ø£Ø¨ÙŠÂ Ù‡Ø±ÙŠØ±Ø©Â Ø±Ø¶ÙŠÂ Ø§Ù„Ù„Ù‡Â Ø¹Ù†Ù‡Â Ù‚Ø§Ù„:"]
Â Â Â Â hadith_train_suffixesÂ =Â ["",Â "Ø±ÙˆØ§Ù‡Â Ø§Ù„Ø¨Ø®Ø§Ø±ÙŠ",Â "Ø±ÙˆØ§Ù‡Â Ù…Ø³Ù„Ù…",Â "Ø­Ø¯ÙŠØ«Â ØµØ­ÙŠØ­",Â "ØµÙ„Ù‰Â Ø§Ù„Ù„Ù‡Â Ø¹Ù„ÙŠÙ‡Â ÙˆØ³Ù„Ù…",Â "Ù…Ù†Â Ø§Ù„Ø³Ù†Ø©Â Ø§Ù„Ù†Ø¨ÙˆÙŠØ©",Â "(Ù…ØªÙÙ‚Â Ø¹Ù„ÙŠÙ‡)",Â "Ø£ÙˆÂ ÙƒÙ…Ø§Â Ù‚Ø§Ù„Â ØµÙ„Ù‰Â Ø§Ù„Ù„Ù‡Â Ø¹Ù„ÙŠÙ‡Â ÙˆØ³Ù„Ù…"]
Â Â Â Â neutral_sentencesÂ =Â ["ÙˆØ¨Ù†Ø§Ø¡Â Ø¹Ù„Ù‰Â Ø°Ù„ÙƒØŒÂ ÙŠÙ…ÙƒÙ†Ù†Ø§Â Ø£Ù†Â Ù†Ø³ØªÙ†ØªØ¬.",Â "ÙˆÙ‡Ø°Ø§Â ÙŠÙˆØ¶Ø­Â Ø¹Ø¸Ù…Ø©Â Ø§Ù„ØªØ´Ø±ÙŠØ¹.",Â "ÙˆÙÙŠÂ Ù‡Ø°Ø§Â Ù‡Ø¯Ø§ÙŠØ©Â Ù„Ù„Ù…Ø¤Ù…Ù†ÙŠÙ†.",Â "Ø¥Ù†Â ÙÙŠÂ Ø°Ù„ÙƒÂ Ù„Ø¢ÙŠØ§ØªÂ Ù„Ù‚ÙˆÙ…Â ÙŠØ¹Ù‚Ù„ÙˆÙ†.",Â "ÙˆÙ‡Ø°Ø§Â Ù‡ÙˆÂ Ø§Ù„Ù‚ÙˆÙ„Â Ø§Ù„Ø±Ø§Ø¬Ø­."]


Â Â Â Â print("ğŸ”„Â PreprocessingÂ trainingÂ examples...")
Â Â Â Â train_examplesÂ =Â []
Â Â Â Â ayah_csv_data,Â hadith_csv_dataÂ =Â [],Â []
Â Â Â Â failed_examplesÂ =Â 0

Â Â Â Â forÂ ayahÂ inÂ tqdm(train_ayah_texts,Â desc="TrainingÂ Ayahs"):
Â Â Â Â Â Â Â Â forÂ variationÂ inÂ range(3):
Â Â Â Â Â Â Â Â Â Â Â Â exampleÂ =Â _create_example_fixed(ayah,Â 'Ayah',Â tokenizer,Â label_to_id,Â quran_train_prefixes,Â quran_train_suffixes,Â neutral_sentences,Â save_details=True)
Â Â Â Â Â Â Â Â Â Â Â Â ifÂ example:
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â train_examples.append({k:Â vÂ forÂ k,Â vÂ inÂ example.items()Â ifÂ kÂ inÂ ["input_ids",Â "attention_mask",Â "labels"]})
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â detailsÂ =Â {k:Â vÂ forÂ k,Â vÂ inÂ example.items()Â ifÂ kÂ notÂ inÂ ["input_ids",Â "attention_mask",Â "labels"]}
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â details.update({"variation_number":Â variationÂ +Â 1,Â "dataset_split":Â "training"})
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â ayah_csv_data.append(details)
Â Â Â Â Â Â Â Â Â Â Â Â else:
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â failed_examplesÂ +=Â 1

Â Â Â Â forÂ hadithÂ inÂ tqdm(train_hadith_texts,Â desc="TrainingÂ Hadiths"):
Â Â Â Â Â Â Â Â forÂ variationÂ inÂ range(3):
Â Â Â Â Â Â Â Â Â Â Â Â exampleÂ =Â _create_example_fixed(hadith,Â 'Hadith',Â tokenizer,Â label_to_id,Â hadith_train_prefixes,Â hadith_train_suffixes,Â neutral_sentences,Â save_details=True)
Â Â Â Â Â Â Â Â Â Â Â Â ifÂ example:
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â train_examples.append({k:Â vÂ forÂ k,Â vÂ inÂ example.items()Â ifÂ kÂ inÂ ["input_ids",Â "attention_mask",Â "labels"]})
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â detailsÂ =Â {k:Â vÂ forÂ k,Â vÂ inÂ example.items()Â ifÂ kÂ notÂ inÂ ["input_ids",Â "attention_mask",Â "labels"]}
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â details.update({"variation_number":Â variationÂ +Â 1,Â "dataset_split":Â "training"})
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â hadith_csv_data.append(details)
Â Â Â Â Â Â Â Â Â Â Â Â else:
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â failed_examplesÂ +=Â 1

Â Â Â Â print(f"âœ…Â GeneratedÂ {len(train_examples)}Â trainingÂ examples")
Â Â Â Â print(f"âŒÂ FailedÂ toÂ createÂ {failed_examples}Â examples")

Â Â Â Â validation_examples,Â validation_csv_dataÂ =Â create_validation_examples(tokenizer,Â label_to_id,Â val_ayah_texts,Â val_hadith_texts)

Â Â Â Â print("ğŸ’¾Â SavingÂ preprocessingÂ detailsÂ toÂ CSVÂ files...")
Â Â Â Â pd.DataFrame(ayah_csv_data).to_csv(os.path.join(CSV_OUTPUT_DIR,Â "ayah_training_details.csv"),Â index=False,Â encoding='utf-8')
Â Â Â Â pd.DataFrame(hadith_csv_data).to_csv(os.path.join(CSV_OUTPUT_DIR,Â "hadith_training_details.csv"),Â index=False,Â encoding='utf-8')
Â Â Â Â pd.DataFrame(validation_csv_data).to_csv(os.path.join(CSV_OUTPUT_DIR,Â "validation_details.csv"),Â index=False,Â encoding='utf-8')
Â Â Â Â print("âœ…Â CSVÂ filesÂ saved.")

Â Â Â Â print("ğŸ’¾Â SavingÂ finalÂ tokenizedÂ datasets...")
Â Â Â Â train_datasetÂ =Â Dataset.from_list(train_examples)
Â Â Â Â val_datasetÂ =Â Dataset.from_list(validation_examples)
Â Â Â Â train_dataset.save_to_disk(PREPROCESSED_TRAIN_PATH)
Â Â Â Â val_dataset.save_to_disk(PREPROCESSED_VAL_PATH)
Â Â Â Â print(f"âœ…Â DatasetsÂ savedÂ toÂ {PREPROCESSED_TRAIN_PATH}Â andÂ {PREPROCESSED_VAL_PATH}")

Â Â Â Â summary_dataÂ =Â [
Â Â Â Â Â Â Â Â {"dataset":Â "Training_Ayah",Â "total_examples":Â len(ayah_csv_data),Â "unique_texts":Â len(train_ayah_texts)},
Â Â Â Â Â Â Â Â {"dataset":Â "Training_Hadith",Â "total_examples":Â len(hadith_csv_data),Â "unique_texts":Â len(train_hadith_texts)},
Â Â Â Â Â Â Â Â {"dataset":Â "Validation_Combined",Â "total_examples":Â len(validation_csv_data),Â "unique_texts":Â len(val_ayah_texts)Â +Â len(val_hadith_texts)},
Â Â Â Â Â Â Â Â {"dataset":Â "TOTAL",Â "total_examples":Â len(train_examples)Â +Â len(validation_examples),Â "failed_examples":Â failed_examples}
Â Â Â Â ]
Â Â Â Â summary_dfÂ =Â pd.DataFrame(summary_data)
Â Â Â Â summary_pathÂ =Â os.path.join(CSV_OUTPUT_DIR,Â "preprocessing_summary.csv")
Â Â Â Â summary_df.to_csv(summary_path,Â index=False)
Â Â Â Â print(f"âœ…Â PreprocessingÂ summaryÂ savedÂ to:Â {summary_path}")
Â Â Â Â print("\nğŸ‰Â PreprocessingÂ complete!")


ifÂ __name__Â ==Â "__main__":
Â Â Â Â main_preprocessing()
