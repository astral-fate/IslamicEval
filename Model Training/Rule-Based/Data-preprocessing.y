#
# SCRIPT 1: preprocessing.py (MAXIMUM COMPATIBILITY VERSION)
#
# Purpose: Load raw Quran and Hadith data, strategically split long Ayahs,
#          aggressively clean all texts, create normalized (Tashkeel-removed) copies
#          of Ayahs for data augmentation, add explicit quotes, and save the final
#          tokenized datasets to disk.
#

import json
import pandas as pd
from datasets import Dataset
import re
import os
import random
from tqdm import tqdm
from transformers import AutoTokenizer
import nltk

# --- 1. Configuration ---
MODEL_NAME = "aubmindlab/bert-base-arabertv2"

# Input data paths
QURAN_JSON_PATH = "/content/drive/MyDrive/FinalIslamic/data/quran.json"
SIX_HADITH_BOOKS_JSON_PATH = "/content/drive/MyDrive/FinalIslamic/data/six_hadith_books.json"

# Output paths for processed data
PREPROCESSED_TRAIN_PATH = "/content/drive/MyDrive/FinalIslamic/prepros/preprocessed_train_dataset"
PREPROCESSED_VAL_PATH = "/content/drive/MyDrive/FinalIslamic/prepros/preprocessed_val_dataset"
CSV_OUTPUT_DIR = "/content/drive/MyDrive/FinalIslamic/preprocessed_csv/"


# --- NEW HELPER FUNCTION ---
def normalize_arabic(text):
    """Removes Arabic diacritics (Tashkeel) and Tatweel from the text."""
    # This regex targets the Unicode range for Arabic diacritics and the Tatweel character.
    text = re.sub(r'[\u064B-\u0652\u0640]', '', text)
    return text


def split_long_texts(texts, tokenizer, max_tokens=25, label_type="Ayah"):
    """
    Splits long texts into smaller chunks based purely on token length.
    It finds the nearest space to the middle of the text to create a clean split.
    """
    print(f"ðŸ”ª Splitting {label_type} texts longer than {max_tokens} tokens...")
    split_texts = []
    split_count = 0
    for text in tqdm(texts, desc=f"Processing {label_type}s"):
        tokens = tokenizer.tokenize(text)
        if len(tokens) <= max_tokens:
            split_texts.append(text)
        else:
            mid_point = len(text) // 2
            split_pos = text.rfind(' ', 0, mid_point)
            if split_pos == -1:
                split_pos = mid_point

            part1 = text[:split_pos].strip()
            part2 = text[split_pos:].strip()

            if part1: split_texts.append(part1)
            if part2: split_texts.append(part2)
            split_count += 1

    print(f"âœ… Splitting complete. Original: {len(texts)} texts, New total: {len(split_texts)} texts. ({split_count} texts were split).")
    return split_texts


def _create_example_fixed(text, label_type, tokenizer, label_to_id, prefixes, suffixes, neutral_sentences, save_details=False):
    """Creates a single tokenized example with context."""
    try:
        cleaned_text = re.sub(r'\s+', ' ', text).strip()
        if not cleaned_text:
            return None

        prefix = random.choice(prefixes)
        suffix = random.choice(suffixes)

        if random.random() > 0.3:
            context = random.choice(neutral_sentences)
            if random.random() > 0.5:
                full_text = f'{prefix} {context} "{cleaned_text}" {suffix}'
            else:
                full_text = f'{prefix} "{cleaned_text}" {context} {suffix}'
        else:
            full_text = f'{prefix} "{cleaned_text}" {suffix}'

        full_text = re.sub(r'\s+', ' ', full_text).strip()
        char_start = full_text.find(cleaned_text)
        if char_start == -1:
            return None
        char_end = char_start + len(cleaned_text)

        tokenized_input = tokenizer(full_text, truncation=True, max_length=512)
        input_ids = tokenized_input['input_ids']
        attention_mask = tokenized_input['attention_mask']
        labels = [label_to_id['O']] * len(input_ids)

        start_token = tokenized_input.char_to_token(char_start)
        end_token = tokenized_input.char_to_token(char_end - 1)

        if start_token is not None and end_token is not None:
            labels[start_token] = label_to_id[f'B-{label_type}']
            for i in range(start_token + 1, min(end_token + 1, len(labels))):
                labels[i] = label_to_id[f'I-{label_type}']

        word_ids = tokenized_input.word_ids()
        final_labels = []
        for i, word_id in enumerate(word_ids):
            if word_id is None or (i > 0 and word_id == word_ids[i - 1]):
                final_labels.append(-100)
            else:
                final_labels.append(labels[i] if i < len(labels) else label_to_id['O'])

        result = {
            "input_ids": input_ids,
            "attention_mask": attention_mask,
            "labels": final_labels
        }

        if save_details:
            result.update({
                "original_text": text,
                "full_text": full_text,
                "prefix": prefix,
                "suffix": suffix,
                "char_start": char_start,
                "char_end": char_end,
                "label_type": label_type,
                "target_span": cleaned_text
            })
        return result
    except Exception:
        return None

def create_validation_examples(tokenizer, label_to_id, val_ayah_texts, val_hadith_texts):
    """Creates validation examples using a different set of patterns to test generalization."""
    print("ðŸ”„ Creating generalization-focused validation examples...")

    val_ayah_prefixes = ["", "ÙˆÙÙŠ Ø§Ù„Ù‚Ø±Ø¢Ù† Ø§Ù„ÙƒØ±ÙŠÙ… Ù†Ø¬Ø¯:", "ÙˆÙ…Ù† Ø¢ÙŠØ§Øª Ø§Ù„Ù„Ù‡:", "ÙˆÙ‚Ø¯ Ø£Ù†Ø²Ù„ Ø§Ù„Ù„Ù‡:", "ÙˆÙŠÙ‚ÙˆÙ„ Ø§Ù„Ø­Ù‚ ØªØ¨Ø§Ø±Ùƒ ÙˆØªØ¹Ø§Ù„Ù‰:", "ÙˆÙÙŠ Ø§Ù„Ø°ÙƒØ± Ø§Ù„Ø­ÙƒÙŠÙ…:", "ÙˆÙÙŠ ÙƒØªØ§Ø¨ Ø§Ù„Ù„Ù‡ Ù†Ù‚Ø±Ø£:", "ÙˆØ§Ù„Ø¯Ù„ÙŠÙ„ Ø¹Ù„Ù‰ Ø°Ù„Ùƒ Ù‚ÙˆÙ„Ù‡ ØªØ¹Ø§Ù„Ù‰:"]
    val_ayah_suffixes = ["", "Ù‡Ø°Ø§ Ù…Ù† ÙƒÙ„Ø§Ù… Ø§Ù„Ù„Ù‡", "Ø¢ÙŠØ© Ø¹Ø¸ÙŠÙ…Ø©", "Ù…Ù† Ø§Ù„Ù‚Ø±Ø¢Ù† Ø§Ù„ÙƒØ±ÙŠÙ…", "ÙƒÙ„Ø§Ù… Ø±Ø¨ Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠÙ†", "Ù…Ù† Ø§Ù„Ø°ÙƒØ± Ø§Ù„Ø­ÙƒÙŠÙ…", "Ø¢ÙŠØ© ÙƒØ±ÙŠÙ…Ø©", "(ØµØ¯Ù‚ Ø§Ù„Ù„Ù‡ Ø§Ù„Ø¹Ø¸ÙŠÙ…)"]
    val_hadith_prefixes = ["", "ÙˆÙÙŠ Ø§Ù„Ø³Ù†Ø© Ø§Ù„Ù†Ø¨ÙˆÙŠØ©:", "ÙˆÙ…Ù† Ù‡Ø¯ÙŠ Ø§Ù„Ù†Ø¨ÙŠ ØµÙ„Ù‰ Ø§Ù„Ù„Ù‡ Ø¹Ù„ÙŠÙ‡ ÙˆØ³Ù„Ù…:", "ÙˆÙ‚Ø¯ Ø¹Ù„Ù…Ù†Ø§ Ø§Ù„Ø±Ø³ÙˆÙ„ ØµÙ„Ù‰ Ø§Ù„Ù„Ù‡ Ø¹Ù„ÙŠÙ‡ ÙˆØ³Ù„Ù…:", "ÙˆÙÙŠ Ø§Ù„Ø­Ø¯ÙŠØ« Ø§Ù„Ø´Ø±ÙŠÙ Ù†Ø¬Ø¯:", "ÙƒÙ…Ø§ Ø¬Ø§Ø¡ ÙÙŠ Ø§Ù„Ø­Ø¯ÙŠØ«:"]
    val_hadith_suffixes = ["", "Ù…Ù† Ø§Ù„Ø³Ù†Ø© Ø§Ù„Ù†Ø¨ÙˆÙŠØ©", "Ø­Ø¯ÙŠØ« Ù†Ø¨ÙˆÙŠ Ø´Ø±ÙŠÙ", "Ù…Ù† Ù‡Ø¯ÙŠ Ø§Ù„Ù…ØµØ·ÙÙ‰", "ØµÙ„Ù‰ Ø§Ù„Ù„Ù‡ Ø¹Ù„ÙŠÙ‡ ÙˆØ³Ù„Ù…", "(Ø±ÙˆØ§Ù‡ Ø§Ù„ØªØ±Ù…Ø°ÙŠ)"]
    val_transitions = ["ÙˆÙ„Ù†ØªØ£Ù…Ù„ Ù…Ø¹Ø§Ù‹", "ÙˆÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ø³ÙŠØ§Ù‚", "ÙˆÙ„Ù„ØªÙˆØ¶ÙŠØ­", "ÙˆØ¥Ù„ÙŠÙƒÙ… Ø§Ù„Ù…Ø«Ø§Ù„", "ÙˆÙÙŠ Ù‡Ø°Ø§ Ø§Ù„ØµØ¯Ø¯", "ÙˆÙ‡Ø°Ø§ ÙŠØ¨ÙŠÙ† Ù„Ù†Ø§ Ø£Ù‡Ù…ÙŠØ© Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹."]

    validation_data = []
    validation_csv_data = []

    for ayah in tqdm(val_ayah_texts, desc="Val Ayahs"):
        for variation_num in range(3):
            example = _create_example_fixed(ayah, 'Ayah', tokenizer, label_to_id, val_ayah_prefixes, val_ayah_suffixes, val_transitions, save_details=True)
            if example:
                validation_data.append({k: v for k, v in example.items() if k in ["input_ids", "attention_mask", "labels"]})
                details = {k: v for k, v in example.items() if k not in ["input_ids", "attention_mask", "labels"]}
                details.update({"variation_number": variation_num + 1, "dataset_split": "validation"})
                validation_csv_data.append(details)

    for hadith in tqdm(val_hadith_texts, desc="Val Hadiths"):
        for variation_num in range(3):
            example = _create_example_fixed(hadith, 'Hadith', tokenizer, label_to_id, val_hadith_prefixes, val_hadith_suffixes, val_transitions, save_details=True)
            if example:
                validation_data.append({k: v for k, v in example.items() if k in ["input_ids", "attention_mask", "labels"]})
                details = {k: v for k, v in example.items() if k not in ["input_ids", "attention_mask", "labels"]}
                details.update({"variation_number": variation_num + 1, "dataset_split": "validation"})
                validation_csv_data.append(details)

    print(f"âœ… Created {len(validation_data)} validation examples.")
    return validation_data, validation_csv_data


def main_preprocessing():
    """Main function to run the entire preprocessing pipeline."""
    print("ðŸ”„ STEP 1: OFFLINE PREPROCESSING")
    print("=" * 50)

    os.makedirs(CSV_OUTPUT_DIR, exist_ok=True)
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    label_list = ['O', 'B-Ayah', 'I-Ayah', 'B-Hadith', 'I-Hadith']
    label_to_id = {l: i for i, l in enumerate(label_list)}

    print("Loading raw data...")
    with open(QURAN_JSON_PATH, 'r', encoding='utf-8') as f:
        quran_data = json.load(f)
    with open(SIX_HADITH_BOOKS_JSON_PATH, 'r', encoding='utf-8') as f:
        six_books_data = json.load(f)

    ayah_texts = [item['ayah_text'] for item in quran_data if 'ayah_text' in item]
    hadith_texts = [item['Matn'].strip() for item in six_books_data if 'Matn' in item and item['Matn'] and item['Matn'].strip()]

    # Step 1: Split long texts
    ayah_texts = split_long_texts(ayah_texts, tokenizer, max_tokens=25, label_type="Ayah")

    # --- NEW: NORMALIZE AND AUGMENT AYAH DATA ---
    print("ðŸ”„ Normalizing Ayah texts for data augmentation...")
    # Create a new list containing Ayahs with Tashkeel removed
    normalized_ayah_texts = [normalize_arabic(text) for text in tqdm(ayah_texts, desc="Normalizing")]

    # Combine the original (with Tashkeel) and normalized (without Tashkeel) lists
    original_count = len(ayah_texts)
    ayah_texts.extend(normalized_ayah_texts)
    print(f"âœ… Normalization complete. Ayah count increased from {original_count} to {len(ayah_texts)}.")
    # --- END OF NEW LOGIC ---

    MAX_TEXT_LENGTH = 1500
    ayah_texts = [t for t in ayah_texts if len(t) < MAX_TEXT_LENGTH]
    hadith_texts = [t for t in hadith_texts if len(t) < MAX_TEXT_LENGTH]
    print(f"Filtered: {len(ayah_texts)} Ayahs, {len(hadith_texts)} Hadiths")

    random.seed(42)
    all_texts = [(text, 'Ayah') for text in ayah_texts] + [(text, 'Hadith') for text in hadith_texts]
    val_subset_size = min(int(len(all_texts) * 0.20), 3333)
    val_texts_subset = random.sample(all_texts, val_subset_size)
    val_ayah_texts = [text for text, label in val_texts_subset if label == 'Ayah']
    val_hadith_texts = [text for text, label in val_texts_subset if label == 'Hadith']

    train_ayah_texts = ayah_texts
    train_hadith_texts = hadith_texts

    quran_train_prefixes = ["", "Ù‚Ø§Ù„ Ø§Ù„Ù„Ù‡ ØªØ¹Ø§Ù„Ù‰:", "ÙˆÙ‚Ø§Ù„ Ø§Ù„Ù„Ù‡ Ø¹Ø² ÙˆØ¬Ù„:", "ÙƒÙ…Ø§ ÙˆØ±Ø¯ ÙÙŠ Ø§Ù„Ù‚Ø±Ø¢Ù† Ø§Ù„ÙƒØ±ÙŠÙ…:", "ÙˆÙÙŠ ÙƒØªØ§Ø¨ Ø§Ù„Ù„Ù‡:", "ÙˆÙ…Ù† Ø¢ÙŠØ§Øª Ø§Ù„Ù„Ù‡:", "ÙŠÙ‚ÙˆÙ„ Ø³Ø¨Ø­Ø§Ù†Ù‡ ÙˆØªØ¹Ø§Ù„Ù‰:", "ÙˆÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ø´Ø£Ù† ÙŠÙ‚ÙˆÙ„ Ø§Ù„Ù„Ù‡:"]
    quran_train_suffixes = ["", "ØµØ¯Ù‚ Ø§Ù„Ù„Ù‡ Ø§Ù„Ø¹Ø¸ÙŠÙ…", "Ø¢ÙŠØ© ÙƒØ±ÙŠÙ…Ø©", "Ù…Ù† Ø§Ù„Ù‚Ø±Ø¢Ù† Ø§Ù„ÙƒØ±ÙŠÙ…", "ÙƒÙ„Ø§Ù… Ø§Ù„Ù„Ù‡ Ø¹Ø² ÙˆØ¬Ù„", "Ù…Ù† Ø§Ù„Ø°ÙƒØ± Ø§Ù„Ø­ÙƒÙŠÙ…", "(Ø³ÙˆØ±Ø© Ø§Ù„Ø¨Ù‚Ø±Ø©ØŒ Ø§Ù„Ø¢ÙŠØ© 255)", "ÙˆÙ‡Ø°Ø§ Ø¨ÙŠØ§Ù† Ù„Ù„Ù†Ø§Ø³"]
    hadith_train_prefixes = ["", "Ù‚Ø§Ù„ Ø±Ø³ÙˆÙ„ Ø§Ù„Ù„Ù‡ ØµÙ„Ù‰ Ø§Ù„Ù„Ù‡ Ø¹Ù„ÙŠÙ‡ ÙˆØ³Ù„Ù…:", "ÙˆÙ‚Ø§Ù„ Ø§Ù„Ù†Ø¨ÙŠ ØµÙ„Ù‰ Ø§Ù„Ù„Ù‡ Ø¹Ù„ÙŠÙ‡ ÙˆØ³Ù„Ù…:", "Ø¹Ù† Ø§Ù„Ù†Ø¨ÙŠ ØµÙ„Ù‰ Ø§Ù„Ù„Ù‡ Ø¹Ù„ÙŠÙ‡ ÙˆØ³Ù„Ù…:", "Ø±ÙˆÙ‰ Ø£Ù† Ø§Ù„Ù†Ø¨ÙŠ ØµÙ„Ù‰ Ø§Ù„Ù„Ù‡ Ø¹Ù„ÙŠÙ‡ ÙˆØ³Ù„Ù… Ù‚Ø§Ù„:", "ÙˆÙÙŠ Ø§Ù„Ø­Ø¯ÙŠØ« Ø§Ù„Ø´Ø±ÙŠÙ:", "ÙˆØ¹Ù† Ø£Ø¨ÙŠ Ù‡Ø±ÙŠØ±Ø© Ø±Ø¶ÙŠ Ø§Ù„Ù„Ù‡ Ø¹Ù†Ù‡ Ù‚Ø§Ù„:"]
    hadith_train_suffixes = ["", "Ø±ÙˆØ§Ù‡ Ø§Ù„Ø¨Ø®Ø§Ø±ÙŠ", "Ø±ÙˆØ§Ù‡ Ù…Ø³Ù„Ù…", "Ø­Ø¯ÙŠØ« ØµØ­ÙŠØ­", "ØµÙ„Ù‰ Ø§Ù„Ù„Ù‡ Ø¹Ù„ÙŠÙ‡ ÙˆØ³Ù„Ù…", "Ù…Ù† Ø§Ù„Ø³Ù†Ø© Ø§Ù„Ù†Ø¨ÙˆÙŠØ©", "(Ù…ØªÙÙ‚ Ø¹Ù„ÙŠÙ‡)", "Ø£Ùˆ ÙƒÙ…Ø§ Ù‚Ø§Ù„ ØµÙ„Ù‰ Ø§Ù„Ù„Ù‡ Ø¹Ù„ÙŠÙ‡ ÙˆØ³Ù„Ù…"]
    neutral_sentences = ["ÙˆØ¨Ù†Ø§Ø¡ Ø¹Ù„Ù‰ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø£Ù† Ù†Ø³ØªÙ†ØªØ¬.", "ÙˆÙ‡Ø°Ø§ ÙŠÙˆØ¶Ø­ Ø¹Ø¸Ù…Ø© Ø§Ù„ØªØ´Ø±ÙŠØ¹.", "ÙˆÙÙŠ Ù‡Ø°Ø§ Ù‡Ø¯Ø§ÙŠØ© Ù„Ù„Ù…Ø¤Ù…Ù†ÙŠÙ†.", "Ø¥Ù† ÙÙŠ Ø°Ù„Ùƒ Ù„Ø¢ÙŠØ§Øª Ù„Ù‚ÙˆÙ… ÙŠØ¹Ù‚Ù„ÙˆÙ†.", "ÙˆÙ‡Ø°Ø§ Ù‡Ùˆ Ø§Ù„Ù‚ÙˆÙ„ Ø§Ù„Ø±Ø§Ø¬Ø­."]


    print("ðŸ”„ Preprocessing training examples...")
    train_examples = []
    ayah_csv_data, hadith_csv_data = [], []
    failed_examples = 0

    for ayah in tqdm(train_ayah_texts, desc="Training Ayahs"):
        for variation in range(3):
            example = _create_example_fixed(ayah, 'Ayah', tokenizer, label_to_id, quran_train_prefixes, quran_train_suffixes, neutral_sentences, save_details=True)
            if example:
                train_examples.append({k: v for k, v in example.items() if k in ["input_ids", "attention_mask", "labels"]})
                details = {k: v for k, v in example.items() if k not in ["input_ids", "attention_mask", "labels"]}
                details.update({"variation_number": variation + 1, "dataset_split": "training"})
                ayah_csv_data.append(details)
            else:
                failed_examples += 1

    for hadith in tqdm(train_hadith_texts, desc="Training Hadiths"):
        for variation in range(3):
            example = _create_example_fixed(hadith, 'Hadith', tokenizer, label_to_id, hadith_train_prefixes, hadith_train_suffixes, neutral_sentences, save_details=True)
            if example:
                train_examples.append({k: v for k, v in example.items() if k in ["input_ids", "attention_mask", "labels"]})
                details = {k: v for k, v in example.items() if k not in ["input_ids", "attention_mask", "labels"]}
                details.update({"variation_number": variation + 1, "dataset_split": "training"})
                hadith_csv_data.append(details)
            else:
                failed_examples += 1

    print(f"âœ… Generated {len(train_examples)} training examples")
    print(f"âŒ Failed to create {failed_examples} examples")

    validation_examples, validation_csv_data = create_validation_examples(tokenizer, label_to_id, val_ayah_texts, val_hadith_texts)

    print("ðŸ’¾ Saving preprocessing details to CSV files...")
    pd.DataFrame(ayah_csv_data).to_csv(os.path.join(CSV_OUTPUT_DIR, "ayah_training_details.csv"), index=False, encoding='utf-8')
    pd.DataFrame(hadith_csv_data).to_csv(os.path.join(CSV_OUTPUT_DIR, "hadith_training_details.csv"), index=False, encoding='utf-8')
    pd.DataFrame(validation_csv_data).to_csv(os.path.join(CSV_OUTPUT_DIR, "validation_details.csv"), index=False, encoding='utf-8')
    print("âœ… CSV files saved.")

    print("ðŸ’¾ Saving final tokenized datasets...")
    train_dataset = Dataset.from_list(train_examples)
    val_dataset = Dataset.from_list(validation_examples)
    train_dataset.save_to_disk(PREPROCESSED_TRAIN_PATH)
    val_dataset.save_to_disk(PREPROCESSED_VAL_PATH)
    print(f"âœ… Datasets saved to {PREPROCESSED_TRAIN_PATH} and {PREPROCESSED_VAL_PATH}")

    summary_data = [
        {"dataset": "Training_Ayah", "total_examples": len(ayah_csv_data), "unique_texts": len(train_ayah_texts)},
        {"dataset": "Training_Hadith", "total_examples": len(hadith_csv_data), "unique_texts": len(train_hadith_texts)},
        {"dataset": "Validation_Combined", "total_examples": len(validation_csv_data), "unique_texts": len(val_ayah_texts) + len(val_hadith_texts)},
        {"dataset": "TOTAL", "total_examples": len(train_examples) + len(validation_examples), "failed_examples": failed_examples}
    ]
    summary_df = pd.DataFrame(summary_data)
    summary_path = os.path.join(CSV_OUTPUT_DIR, "preprocessing_summary.csv")
    summary_df.to_csv(summary_path, index=False)
    print(f"âœ… Preprocessing summary saved to: {summary_path}")
    print("\nðŸŽ‰ Preprocessing complete!")


if __name__ == "__main__":
    main_preprocessing()
