import json
import pandas as pd
from datasets import Dataset
import re
import os
import random
from tqdm import tqdm
from transformers import AutoTokenizer
import nltk

# --- 1. Configuration ---
MODEL_NAME = "aubmindlab/bert-base-arabertv2"

# Input data paths
QURAN_JSON_PATH = "/content/drive/MyDrive/FinalIslamic/data/quran.json"
SIX_HADITH_BOOKS_JSON_PATH = "/content/drive/MyDrive/FinalIslamic/data/six_hadith_books.json"

# Output paths for processed data
PREPROCESSED_TRAIN_PATH = "/content/drive/MyDrive/FinalIslamic/prepros/preprocessed_train_full"
PREPROCESSED_VAL_PATH = "/content/drive/MyDrive/FinalIslamic/prepros/preprocessed_val_full"
CSV_OUTPUT_DIR = "/content/drive/MyDrive/FinalIslamic/preprocessed_csv_10k_full/"


# --- NEW HELPER FUNCTION ---
def normalize_arabic(text):
    """Removes Arabic diacritics (Tashkeel) and Tatweel from the text."""
    # This regex targets the Unicode range for Arabic diacritics and the Tatweel character.
    text = re.sub(r'[\u064B-\u0652\u0640]', '', text)
    return text


def split_long_texts(texts, tokenizer, max_tokens=25, label_type="Ayah"):
    """
    Splits long texts into smaller chunks based purely on token length.
    It finds the nearest space to the middle of the text to create a clean split.
    """
    print(f"ðŸ”ª Splitting {label_type} texts longer than {max_tokens} tokens...")
    split_texts = []
    split_count = 0
    for text in tqdm(texts, desc=f"Processing {label_type}s"):
        tokens = tokenizer.tokenize(text)
        if len(tokens) <= max_tokens:
            split_texts.append(text)
        else:
            mid_point = len(text) // 2
            split_pos = text.rfind(' ', 0, mid_point)
            if split_pos == -1:
                split_pos = mid_point

            part1 = text[:split_pos].strip()
            part2 = text[split_pos:].strip()

            if part1: split_texts.append(part1)
            if part2: split_texts.append(part2)
            split_count += 1

    print(f"âœ… Splitting complete. Original: {len(texts)} texts, New total: {len(split_texts)} texts. ({split_count} texts were split).")
    return split_texts


def _create_example_fixed(text, label_type, tokenizer, label_to_id, prefixes, suffixes, neutral_sentences, save_details=False):
    """Creates a single tokenized example with context."""
    try:
        cleaned_text = re.sub(r'\s+', ' ', text).strip()
        if not cleaned_text:
            return None

        prefix = random.choice(prefixes)
        suffix = random.choice(suffixes)

        if random.random() > 0.3:
            context = random.choice(neutral_sentences)
            if random.random() > 0.5:
                full_text = f'{prefix} {context} "{cleaned_text}" {suffix}'
            else:
                full_text = f'{prefix} "{cleaned_text}" {context} {suffix}'
        else:
            full_text = f'{prefix} "{cleaned_text}" {suffix}'

        full_text = re.sub(r'\s+', ' ', full_text).strip()
        char_start = full_text.find(cleaned_text)
        if char_start == -1:
            return None
        char_end = char_start + len(cleaned_text)

        tokenized_input = tokenizer(full_text, truncation=True, max_length=512)
        input_ids = tokenized_input['input_ids']
        attention_mask = tokenized_input['attention_mask']
        labels = [label_to_id['O']] * len(input_ids)

        start_token = tokenized_input.char_to_token(char_start)
        end_token = tokenized_input.char_to_token(char_end - 1)

        if start_token is not None and end_token is not None:
            labels[start_token] = label_to_id[f'B-{label_type}']
            for i in range(start_token + 1, min(end_token + 1, len(labels))):
                labels[i] = label_to_id[f'I-{label_type}']

        word_ids = tokenized_input.word_ids()
        final_labels = []
        for i, word_id in enumerate(word_ids):
            if word_id is None or (i > 0 and word_id == word_ids[i - 1]):
                final_labels.append(-100)
            else:
                final_labels.append(labels[i] if i < len(labels) else label_to_id['O'])

        result = {
            "input_ids": input_ids,
            "attention_mask": attention_mask,
            "labels": final_labels
        }

        if save_details:
            result.update({
                "original_text": text,
                "full_text": full_text,
                "prefix": prefix,
                "suffix": suffix,
                "char_start": char_start,
                "char_end": char_end,
                "label_type": label_type,
                "target_span": cleaned_text
            })
        return result
    except Exception:
        return None

def create_validation_examples(tokenizer, label_to_id, val_ayah_texts, val_hadith_texts):
    """Creates validation examples using a different set of patterns to test generalization."""
    print("ðŸ”„ Creating generalization-focused validation examples...")

    val_ayah_prefixes = ["", "ÙˆÙÙŠ Ø§Ù„Ù‚Ø±Ø¢Ù† Ø§Ù„ÙƒØ±ÙŠÙ… Ù†Ø¬Ø¯:", "ÙˆÙ…Ù† Ø¢ÙŠØ§Øª Ø§Ù„Ù„Ù‡:", "ÙˆÙ‚Ø¯ Ø£Ù†Ø²Ù„ Ø§Ù„Ù„Ù‡:", "ÙˆÙŠÙ‚ÙˆÙ„ Ø§Ù„Ø­Ù‚ ØªØ¨Ø§Ø±Ùƒ ÙˆØªØ¹Ø§Ù„Ù‰:", "ÙˆÙÙŠ Ø§Ù„Ø°ÙƒØ± Ø§Ù„Ø­ÙƒÙŠÙ…:", "ÙˆÙÙŠ ÙƒØªØ§Ø¨ Ø§Ù„Ù„Ù‡ Ù†Ù‚Ø±Ø£:", "ÙˆØ§Ù„Ø¯Ù„ÙŠÙ„ Ø¹Ù„Ù‰ Ø°Ù„Ùƒ Ù‚ÙˆÙ„Ù‡ ØªØ¹Ø§Ù„Ù‰:"]
    val_ayah_suffixes = ["", "Ù‡Ø°Ø§ Ù…Ù† ÙƒÙ„Ø§Ù… Ø§Ù„Ù„Ù‡", "Ø¢ÙŠØ© Ø¹Ø¸ÙŠÙ…Ø©", "Ù…Ù† Ø§Ù„Ù‚Ø±Ø¢Ù† Ø§Ù„ÙƒØ±ÙŠÙ…", "ÙƒÙ„Ø§Ù… Ø±Ø¨ Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠÙ†", "Ù…Ù† Ø§Ù„Ø°ÙƒØ± Ø§Ù„Ø­ÙƒÙŠÙ…", "Ø¢ÙŠØ© ÙƒØ±ÙŠÙ…Ø©", "(ØµØ¯Ù‚ Ø§Ù„Ù„Ù‡ Ø§Ù„Ø¹Ø¸ÙŠÙ…)"]
    val_hadith_prefixes = ["", "ÙˆÙÙŠ Ø§Ù„Ø³Ù†Ø© Ø§Ù„Ù†Ø¨ÙˆÙŠØ©:", "ÙˆÙ…Ù† Ù‡Ø¯ÙŠ Ø§Ù„Ù†Ø¨ÙŠ ØµÙ„Ù‰ Ø§Ù„Ù„Ù‡ Ø¹Ù„ÙŠÙ‡ ÙˆØ³Ù„Ù…:", "ÙˆÙ‚Ø¯ Ø¹Ù„Ù…Ù†Ø§ Ø§Ù„Ø±Ø³ÙˆÙ„ ØµÙ„Ù‰ Ø§Ù„Ù„Ù‡ Ø¹Ù„ÙŠÙ‡ ÙˆØ³Ù„Ù…:", "ÙˆÙÙŠ Ø§Ù„Ø­Ø¯ÙŠØ« Ø§Ù„Ø´Ø±ÙŠÙ Ù†Ø¬Ø¯:", "ÙƒÙ…Ø§ Ø¬Ø§Ø¡ ÙÙŠ Ø§Ù„Ø­Ø¯ÙŠØ«:"]
    val_hadith_suffixes = ["", "Ù…Ù† Ø§Ù„Ø³Ù†Ø© Ø§Ù„Ù†Ø¨ÙˆÙŠØ©", "Ø­Ø¯ÙŠØ« Ù†Ø¨ÙˆÙŠ Ø´Ø±ÙŠÙ", "Ù…Ù† Ù‡Ø¯ÙŠ Ø§Ù„Ù…ØµØ·ÙÙ‰", "ØµÙ„Ù‰ Ø§Ù„Ù„Ù‡ Ø¹Ù„ÙŠÙ‡ ÙˆØ³Ù„Ù…", "(Ø±ÙˆØ§Ù‡ Ø§Ù„ØªØ±Ù…Ø°ÙŠ)"]
    val_transitions = ["ÙˆÙ„Ù†ØªØ£Ù…Ù„ Ù…Ø¹Ø§Ù‹", "ÙˆÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ø³ÙŠØ§Ù‚", "ÙˆÙ„Ù„ØªÙˆØ¶ÙŠØ­", "ÙˆØ¥Ù„ÙŠÙƒÙ… Ø§Ù„Ù…Ø«Ø§Ù„", "ÙˆÙÙŠ Ù‡Ø°Ø§ Ø§Ù„ØµØ¯Ø¯", "ÙˆÙ‡Ø°Ø§ ÙŠØ¨ÙŠÙ† Ù„Ù†Ø§ Ø£Ù‡Ù…ÙŠØ© Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹."]

    
    
    validation_data = []
    validation_csv_data = []

    for ayah in tqdm(val_ayah_texts, desc="Val Ayahs"):
        for variation_num in range(3):
            example = _create_example_fixed(ayah, 'Ayah', tokenizer, label_to_id, val_ayah_prefixes, val_ayah_suffixes, val_transitions, save_details=True)
            if example:
                validation_data.append({k: v for k, v in example.items() if k in ["input_ids", "attention_mask", "labels"]})
                details = {k: v for k, v in example.items() if k not in ["input_ids", "attention_mask", "labels"]}
                details.update({"variation_number": variation_num + 1, "dataset_split": "validation"})
                validation_csv_data.append(details)

    for hadith in tqdm(val_hadith_texts, desc="Val Hadiths"):
        for variation_num in range(3):
            example = _create_example_fixed(hadith, 'Hadith', tokenizer, label_to_id, val_hadith_prefixes, val_hadith_suffixes, val_transitions, save_details=True)
            if example:
                validation_data.append({k: v for k, v in example.items() if k in ["input_ids", "attention_mask", "labels"]})
                details = {k: v for k, v in example.items() if k not in ["input_ids", "attention_mask", "labels"]}
                details.update({"variation_number": variation_num + 1, "dataset_split": "validation"})
                validation_csv_data.append(details)

    print(f"âœ… Created {len(validation_data)} validation examples.")
    return validation_data, validation_csv_data


def main_preprocessing():
    """Main function to run the entire preprocessing pipeline."""
    print("ðŸ”„ STEP 1: OFFLINE PREPROCESSING")
    print("=" * 50)

    os.makedirs(CSV_OUTPUT_DIR, exist_ok=True)
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    label_list = ['O', 'B-Ayah', 'I-Ayah', 'B-Hadith', 'I-Hadith']
    label_to_id = {l: i for i, l in enumerate(label_list)}

    print("Loading raw data...")
    with open(QURAN_JSON_PATH, 'r', encoding='utf-8') as f:
        quran_data = json.load(f)
    with open(SIX_HADITH_BOOKS_JSON_PATH, 'r', encoding='utf-8') as f:
        six_books_data = json.load(f)

    original_ayah_texts = [item['ayah_text'] for item in quran_data if 'ayah_text' in item]
    original_hadith_texts = [item['Matn'].strip() for item in six_books_data if 'Matn' in item and item['Matn'] and item['Matn'].strip()]

    # --- Process the *entire* dataset first (splitting long texts and normalizing) ---
    print("ðŸ”„ Processing entire dataset (splitting long texts and normalizing)...")

    # Step 1: Split long texts for the entire dataset
    processed_ayah_texts = split_long_texts(original_ayah_texts, tokenizer, max_tokens=25, label_type="Ayah")
    processed_hadith_texts = split_long_texts(original_hadith_texts, tokenizer, max_tokens=25, label_type="Hadith")

    # Step 2: Normalize Ayah texts for data augmentation
    print("ðŸ”„ Normalizing processed Ayah texts for data augmentation...")
    normalized_processed_ayah_texts = [normalize_arabic(text) for text in tqdm(processed_ayah_texts, desc="Normalizing")]
    processed_ayah_texts.extend(normalized_processed_ayah_texts)
    print(f"âœ… Normalization complete. Processed Ayah count increased to {len(processed_ayah_texts)}.")

    # Step 3: Filter out very long texts from the processed set
    MAX_TEXT_LENGTH = 1500
    processed_ayah_texts = [t for t in processed_ayah_texts if len(t) < MAX_TEXT_LENGTH]
    processed_hadith_texts = [t for t in processed_hadith_texts if len(t) < MAX_TEXT_LENGTH]
    print(f"Filtered processed texts (length < {MAX_TEXT_LENGTH}): {len(processed_ayah_texts)} Ayahs, {len(processed_hadith_texts)} Hadiths")


    random.seed(42)

    # --- Sample validation set from the processed data (balanced 10% Ayah, 10% Hadith) ---
    val_ayah_size = int(len(processed_ayah_texts) * 0.10)
    val_hadith_size = int(len(processed_hadith_texts) * 0.10)

    val_ayah_texts = random.sample(processed_ayah_texts, val_ayah_size)
    val_hadith_texts = random.sample(processed_hadith_texts, val_hadith_size)

    print(f"Sampled {len(val_ayah_texts)} processed Ayahs and {len(val_hadith_texts)} processed Hadiths for validation.")

    # --- Use remaining processed data for training ---
    train_ayah_texts = [text for text in processed_ayah_texts if text not in val_ayah_texts]
    train_hadith_texts = [text for text in processed_hadith_texts if text not in val_hadith_texts]

    print(f"Using {len(train_ayah_texts)} processed Ayahs and {len(train_hadith_texts)} processed Hadiths for training.")


    quran_train_prefixes = [
        "",
        "ÙˆÙ…Ù† Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù‚Ø±Ø¢Ù†ÙŠØ©:",
        "ÙˆØ¬Ø§Ø¡ ÙÙŠ Ø§Ù„Ù…ØµØ­Ù Ø§Ù„Ø´Ø±ÙŠÙ:",
        "ÙˆÙ…Ù† Ø¢ÙŠØ§Øª Ø§Ù„Ø°ÙƒØ± Ø§Ù„Ø­ÙƒÙŠÙ…:",
        "ÙˆÙÙŠ Ø§Ù„ØªÙ†Ø²ÙŠÙ„ Ø§Ù„Ø­ÙƒÙŠÙ…:",
        "ÙˆÙ…Ù…Ø§ Ø£Ù†Ø²Ù„ Ø¹Ù„Ù‰ Ù…Ø­Ù…Ø¯:",
        "ÙˆÙÙŠ Ø§Ù„ÙˆØ­ÙŠ Ø§Ù„Ù…Ø¨ÙŠÙ†:",
        "ÙˆÙ…Ù† ÙƒÙ„Ø§Ù… Ø±Ø¨ Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠÙ†:",
        "ÙˆÙÙŠ Ø§Ù„Ø³ÙˆØ± Ø§Ù„ÙƒØ±ÙŠÙ…Ø©:",
        "ÙˆÙ…Ù† Ø§Ù„Ù…Ø­ÙƒÙ… ÙˆØ§Ù„Ù…ØªØ´Ø§Ø¨Ù‡:",
        "ÙˆØ¨ÙŠÙ† Ø§Ù„Ø¢ÙŠØ§Øª Ø§Ù„Ø¨ÙŠÙ†Ø§Øª:",
        "ÙˆÙ…Ù† Ø§Ù„ØªØ´Ø±ÙŠØ¹ Ø§Ù„Ø¥Ù„Ù‡ÙŠ:",
        "ÙˆÙÙŠ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ù‚Ø¯Ø³:",
        "ÙˆÙ…Ù† Ø§Ù„Ø­Ù‚ Ø§Ù„Ù…Ù†Ø²Ù„:"
    ]

    quran_train_suffixes = [
        "",
        "Ù…Ù† Ø§Ù„Ø¢ÙŠØ§Øª Ø§Ù„Ù…Ø­ÙƒÙ…Ø§Øª",
        "ÙˆÙ‡Ùˆ Ø§Ù„Ø­Ù‚ Ø§Ù„Ù…Ø¨ÙŠÙ†",
        "Ù…Ù† ÙƒØªØ§Ø¨ Ø§Ù„Ù„Ù‡ Ø§Ù„Ù…Ø¬ÙŠØ¯",
        "ÙˆØ§Ù„Ø§Ù„Ù„Ù‡ Ø£Ø¹Ù„Ù… Ø¨Ù…Ø§ Ø£Ù†Ø²Ù„",
        "ÙˆÙÙŠ Ø°Ù„Ùƒ Ù‡Ø¯Ù‰ Ù„Ù„Ù…ØªÙ‚ÙŠÙ†",
        "Ù…Ù† Ø§Ù„Ù†ÙˆØ± Ø§Ù„Ù…Ø¨ÙŠÙ†",
        "ÙˆØ°Ù„Ùƒ Ù…Ù† ÙØ¶Ù„ Ø§Ù„Ù„Ù‡",
        "Ù…Ù† Ø§Ù„Ø­ÙƒÙ…Ø© Ø§Ù„Ø¥Ù„Ù‡ÙŠØ©",
        "ÙˆÙ‡Ùˆ Ø§Ù„Ù‚ÙˆÙ„ Ø§Ù„ÙØµÙ„",
        "Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù† Ø§Ù„ÙˆØ§Ø¶Ø­",
        "ÙˆÙÙŠÙ‡ Ø´ÙØ§Ø¡ Ù„Ù„Ù†ÙÙˆØ³",
        "Ù…Ù† Ø§Ù„Ù‡Ø¯Ù‰ ÙˆØ§Ù„Ù†ÙˆØ±",
        "ÙˆØ§Ù„Ù„Ù‡ Ø¹Ù„Ù‰ ÙƒÙ„ Ø´ÙŠØ¡ Ù‚Ø¯ÙŠØ±"
    ]

    hadith_train_prefixes = [
        "",
        "ÙˆÙ…Ù† Ø§Ù„Ø£Ø­Ø§Ø¯ÙŠØ« Ø§Ù„Ù†Ø¨ÙˆÙŠØ©:",
        "ÙˆÙÙŠ Ø§Ù„Ø³Ù†Ø© Ø§Ù„Ù…Ø·Ù‡Ø±Ø©:",
        "ÙˆÙ…Ù† ÙƒÙ„Ø§Ù… Ø§Ù„Ù…ØµØ·ÙÙ‰:",
        "ÙˆØ¬Ø§Ø¡ ÙÙŠ Ø§Ù„Ø£Ø«Ø±:",
        "ÙˆÙ…Ù† Ù‡Ø¯ÙŠ Ø§Ù„Ù†Ø¨ÙŠ Ø§Ù„ÙƒØ±ÙŠÙ…:",
        "ÙˆÙÙŠ Ø§Ù„ØªÙˆØ¬ÙŠÙ‡ Ø§Ù„Ù†Ø¨ÙˆÙŠ:",
        "ÙˆÙ…Ù† ØªØ¹Ù„ÙŠÙ… Ø§Ù„Ø±Ø³ÙˆÙ„:",
        "ÙˆÙÙŠ Ø§Ù„Ø¥Ø±Ø´Ø§Ø¯ Ø§Ù„Ù…Ø­Ù…Ø¯ÙŠ:",
        "ÙˆÙ…Ù† Ø§Ù„Ø¨ÙŠØ§Ù† Ø§Ù„Ù†Ø¨ÙˆÙŠ:",
        "ÙˆÙÙŠ Ø§Ù„ØªØ±Ø¨ÙŠØ© Ø§Ù„Ø¥Ø³Ù„Ø§Ù…ÙŠØ©:",
        "ÙˆÙ…Ù† Ø§Ù„Ø­ÙƒÙ…Ø© Ø§Ù„Ù†Ø¨ÙˆÙŠØ©:",
        "ÙˆÙÙŠ Ø§Ù„Ù…Ù†Ù‡Ø¬ Ø§Ù„Ù…Ø­Ù…Ø¯ÙŠ:",
        "ÙˆÙ…Ù…Ø§ Ø¹Ù„Ù… Ø§Ù„Ù†Ø¨ÙŠ Ø£Ù…ØªÙ‡:",
        "ÙˆÙÙŠ Ø§Ù„ØªÙˆØ¶ÙŠØ­ Ø§Ù„Ø´Ø±ÙŠÙ:"
    ]

    hadith_train_suffixes = [
        "",
        "Ù…Ù† Ø§Ù„ØªÙˆØ¬ÙŠÙ‡ Ø§Ù„Ù†Ø¨ÙˆÙŠ",
        "ÙˆÙ‡Ùˆ Ù…Ù† Ø§Ù„Ù‡Ø¯ÙŠ Ø§Ù„Ø´Ø±ÙŠÙ",
        "Ù…Ù† Ø§Ù„Ù…Ù†Ù‡Ø¬ Ø§Ù„Ø¥Ø³Ù„Ø§Ù…ÙŠ",
        "ÙˆØ§Ù„Ù„Ù‡ ÙˆØ±Ø³ÙˆÙ„Ù‡ Ø£Ø¹Ù„Ù…",
        "Ù…Ù† Ø§Ù„ØªØ±Ø¨ÙŠØ© Ø§Ù„Ù†Ø¨ÙˆÙŠØ©",
        "ÙˆÙÙŠÙ‡ Ø§Ù„Ø­ÙƒÙ…Ø© Ø§Ù„Ø¨Ø§Ù„ØºØ©",
        "Ù…Ù† Ø§Ù„Ø³Ù†Ø© Ø§Ù„Ù…Ø¨Ø§Ø±ÙƒØ©",
        "ÙˆØ°Ù„Ùƒ Ù…Ù† ÙƒÙ…Ø§Ù„ Ø§Ù„Ø¯ÙŠÙ†",
        "Ù…Ù† Ø§Ù„ØªØ¹Ù„ÙŠÙ… Ø§Ù„Ù…Ø­Ù…Ø¯ÙŠ",
        "ÙˆÙ‡Ùˆ Ù…Ù† Ø§Ù„Ø¥Ø±Ø´Ø§Ø¯ Ø§Ù„Ø­ÙƒÙŠÙ…",
        "Ù…Ù† Ø§Ù„Ø£Ø®Ù„Ø§Ù‚ Ø§Ù„ÙØ§Ø¶Ù„Ø©",
        "ÙˆÙÙŠ Ø°Ù„Ùƒ Ù‚Ø¯ÙˆØ© Ø­Ø³Ù†Ø©",
        "Ù…Ù† Ø§Ù„Ù…Ø¨Ø§Ø¯Ø¦ Ø§Ù„Ø¥Ø³Ù„Ø§Ù…ÙŠØ©",
        "ÙˆØ§Ù„Ø­Ù…Ø¯ Ù„Ù„Ù‡ Ø±Ø¨ Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠÙ†"
    ]

    neutral_sentences = [
        "ÙˆÙ…Ù† Ø®Ù„Ø§Ù„ Ù‡Ø°Ø§ Ø§Ù„Ù†Øµ ÙŠØªØ¶Ø­ Ù„Ù†Ø§.",
        "ÙˆÙ‡Ø°Ø§ Ù…Ø§ ÙŠØ¤ÙƒØ¯ Ø¹Ù„Ù‰ Ø£Ù‡Ù…ÙŠØ© Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹.",
        "ÙˆÙŠÙ…ÙƒÙ† Ø§Ù„Ø§Ø³ØªÙØ§Ø¯Ø© Ù…Ù† Ù‡Ø°Ø§ ÙÙŠ Ø­ÙŠØ§ØªÙ†Ø§.",
        "ÙˆÙ‡Ø°Ø§ ÙŠØ¯Ù„ Ø¹Ù„Ù‰ Ø¹Ù…Ù‚ Ø§Ù„ØªØ´Ø±ÙŠØ¹.",
        "ÙˆÙ…Ù† Ù‡Ù†Ø§ Ù†ÙÙ‡Ù… Ø§Ù„Ø­ÙƒÙ…Ø© Ø§Ù„Ø¥Ù„Ù‡ÙŠØ©.",
        "ÙˆÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ø³ÙŠØ§Ù‚ ÙŠØ¬Ø¨ Ø£Ù† Ù†ØªØ£Ù…Ù„.",
        "ÙˆÙ‡Ø°Ø§ Ø§Ù„Ù…Ø¹Ù†Ù‰ ÙŠØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØ¯Ø¨Ø±.",
        "ÙˆÙ…Ù† Ù‡Ø°Ø§ Ø§Ù„Ù…Ù†Ø·Ù„Ù‚ Ù†Ø³ØªØ·ÙŠØ¹ Ø§Ù„Ù‚ÙˆÙ„.",
        "ÙˆØ¨Ù†Ø§Ø¡ Ø¹Ù„Ù‰ Ù…Ø§ Ø³Ø¨Ù‚ Ù†Ø®Ù„Øµ Ø¥Ù„Ù‰.",
        "ÙˆÙÙŠ Ø¶ÙˆØ¡ Ù‡Ø°Ø§ Ø§Ù„ÙÙ‡Ù… ÙŠÙ…ÙƒÙ†Ù†Ø§.",
        "ÙˆÙ‡Ø°Ø§ ÙŠÙ‚ÙˆØ¯Ù†Ø§ Ø¥Ù„Ù‰ Ù†ØªÙŠØ¬Ø© Ù…Ù‡Ù…Ø©.",
        "ÙˆÙ…Ù† Ø®Ù„Ø§Ù„ Ø§Ù„ØªØ£Ù…Ù„ ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù†Øµ.",
        "ÙˆÙ‡Ø°Ø§ Ø§Ù„Ø£Ù…Ø± ÙŠØ³ØªØ¯Ø¹ÙŠ Ù…Ù†Ø§ Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡.",
        "ÙˆØ¨Ù‡Ø°Ø§ Ø§Ù„Ù…ÙÙ‡ÙˆÙ… Ù†ØµÙ„ Ø¥Ù„Ù‰ Ø§Ù„Ù‡Ø¯Ù.",
        "ÙˆÙ…Ù† Ù‡Ø°Ù‡ Ø§Ù„Ø²Ø§ÙˆÙŠØ© Ù†Ù†Ø¸Ø± Ù„Ù„Ù…Ø³Ø£Ù„Ø©."
    ]

    print("ðŸ”„ Preprocessing training examples...")
    train_examples = []
    ayah_csv_data, hadith_csv_data = [], []
    failed_examples = 0

    for ayah in tqdm(train_ayah_texts, desc="Training Ayahs"):
        for variation in range(3):
            example = _create_example_fixed(ayah, 'Ayah', tokenizer, label_to_id, quran_train_prefixes, quran_train_suffixes, neutral_sentences, save_details=True)
            if example:
                train_examples.append({k: v for k, v in example.items() if k in ["input_ids", "attention_mask", "labels"]})
                details = {k: v for k, v in example.items() if k not in ["input_ids", "attention_mask", "labels"]}
                details.update({"variation_number": variation + 1, "dataset_split": "training"})
                ayah_csv_data.append(details)
            else:
                failed_examples += 1

    for hadith in tqdm(train_hadith_texts, desc="Training Hadiths"):
        for variation in range(3):
            example = _create_example_fixed(hadith, 'Hadith', tokenizer, label_to_id, hadith_train_prefixes, hadith_train_suffixes, neutral_sentences, save_details=True)
            if example:
                train_examples.append({k: v for k, v in example.items() if k in ["input_ids", "attention_mask", "labels"]})
                details = {k: v for k, v in example.items() if k not in ["input_ids", "attention_mask", "labels"]}
                details.update({"variation_number": variation + 1, "dataset_split": "training"})
                hadith_csv_data.append(details)
            else:
                failed_examples += 1

    print(f"âœ… Generated {len(train_examples)} training examples")
    print(f"âŒ Failed to create {failed_examples} examples")

    # Use the sampled validation texts with custom validation templates
    validation_examples, validation_csv_data = create_validation_examples(tokenizer, label_to_id, val_ayah_texts, val_hadith_texts)

    print("ðŸ’¾ Saving preprocessing details to CSV files...")
    pd.DataFrame(ayah_csv_data).to_csv(os.path.join(CSV_OUTPUT_DIR, "ayah_training_details.csv"), index=False, encoding='utf-8')
    pd.DataFrame(hadith_csv_data).to_csv(os.path.join(CSV_OUTPUT_DIR, "hadith_training_details.csv"), index=False, encoding='utf-8')
    pd.DataFrame(validation_csv_data).to_csv(os.path.join(CSV_OUTPUT_DIR, "validation_details.csv"), index=False, encoding='utf-8')
    print("âœ… CSV files saved.")

    print("ðŸ’¾ Saving final tokenized datasets...")
    train_dataset = Dataset.from_list(train_examples)
    val_dataset = Dataset.from_list(validation_examples)
    train_dataset.save_to_disk(PREPROCESSED_TRAIN_PATH)
    val_dataset.save_to_disk(PREPROCESSED_VAL_PATH)
    print(f"âœ… Datasets saved to {PREPROCESSED_TRAIN_PATH} and {PREPROCESSED_VAL_PATH}")

    summary_data = [
        {"dataset": "Training_Ayah", "total_examples": len(ayah_csv_data), "unique_texts": len(train_ayah_texts)},
        {"dataset": "Training_Hadith", "total_examples": len(hadith_csv_data), "unique_texts": len(train_hadith_texts)},
        {"dataset": "Validation_Combined", "total_examples": len(validation_csv_data), "unique_texts": len(val_ayah_texts) + len(val_hadith_texts)},
        {"dataset": "TOTAL", "total_examples": len(train_examples) + len(validation_examples), "failed_examples": failed_examples}
    ]
    summary_df = pd.DataFrame(summary_data)
    summary_path = os.path.join(CSV_OUTPUT_DIR, "preprocessing_summary.csv")
    summary_df.to_csv(summary_path, index=False)
    print(f"âœ… Preprocessing summary saved to: {summary_path}")
    print("\nðŸŽ‰ Preprocessing complete!")


if __name__ == "__main__":
    main_preprocessing()
